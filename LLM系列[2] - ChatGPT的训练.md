# LLM系列[2] - ChatGPT的训练

[TOC]

> 参考资料: 
>
> Backbone : [【LLM】从零开始训练大模型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/636270877)
>
> Information : [如何从零开始训练大模型（minicpm分享&讨论） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/686664720)
>
> Information : [LLM-SFT-trick - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/682604566)
>
> Information : [【LLM 加速技巧】Muti Query Attention 和 Attention with Linear Bias附源码](https://zhuanlan.zhihu.com/p/634236135)
>
> Information : [如何更好地继续预训练（Continue PreTraining） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/654463331)
>
> Information : [ChatGPT是怎样被训练出来的？ - 知乎 (zhihu.com)](https://www.zhihu.com/zvideo/1584941670507896832) -> Instruct GPT pipline
>
> Information : [LLM-SFT-trick - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/682604566)

ChatGPT的模型训练通常分为以下四个步骤:

* **第一阶段 - 预训练[Pre-Training]**: 

  在这一阶段, 模型基于海量的无标签文本数据进行预训练. 通过使用**自监督学习[Self-Supervised Learning]**的方法, 如**下一个词预测[Next Word Prediction]**或掩码语言模型[Masked Language Model, MLM]等, 模型可以学习到**语言的基本结构和语义信息**. 这一阶段的目的是让模型获取大规模语料中蕴含的"知识". 

  预训练完成后, 我们得到了一个**基础模型[Base Model]**, 它具备了一定的语言理解和生成能力, 可以对用户输入进行初步的"续写".

* **第二阶段 - 指令微调[Instruction Tuning]**: 

  > Instruct GPT: 
  >
  > * GPT-3 architecture
  > * 16 epochs
  > * residual dropout of 0.2
  > * 1.3B & 6B: LR of 9.65e-6 and a batch size of 32.
  > * 175B: LR of 5.03e-6 and a batch size of 8.

  在这一阶段, 我们使用带有指令标签的问答数据(人工生成或机器合成)对基础模型进行微调. 通过引入人类的指令和意图, 模型学习如何理解和执行这些指令, 以生成符合人类期望的回答. 这个过程可以看作是对基础模型进行**指令对齐[Instruction Alignment]**, 通过学习如何根据指令生成回答,模型能够更好地理解人类的意图,并以对齐人类偏好的方式进行交互。这个阶段同样使用预测下一个词的生成方法, 但是优化的目标是尽量最小化指令回答的生成误差. 

  指令微调阶段完成后, 基础模型被转化为**对话模型[Chat Model]**, 它可以与人类进行问答式的对话交互.

* **第三阶段 - 奖励模型训练[Reward Model Training]**: 

  > Instruct GPT: 
  >
  > * 6B | 175B会不稳定。
  > * 模型最后替换为投影层，输出标量值。

  尽管对话模型已经能够生成**形式上符合指令**的回答, 但它并**不清楚**自己生成的答案的优劣. 为了进一步提高模型生成高质量答案的能力, 我们需要引入奖励模型[Reward Model, RM]. **奖励模型是一个独立于对话模型的神经网络**, 它接受人类对模型生成答案的偏好标注数据进行训练, 学习评估答案的优劣. 

  通过**奖励模型[Reward Model, RM]**, 我们可以为对话模型的输出提供一个质量评估的标准.

* **第四阶段 - 强化学习[Reinforcement Learning]**: 

  > Instruct GPT:
  >
  > * GPT-3 Architecture
  > * 2 epochs
  > * mix in 10% pretraining data

  有了Chat模型和奖励模型,我们就可以使用强化学习的方法对Chat模型进行进一步优化。

  具体来说,我们将**Chat模型作为策略网络[Policy Network]**,将**奖励模型作为价值网络[Value Network]**,通过**策略梯度[Policy Gradient]算法来更新Chat模型的参数**。在这个过程中,Chat模型学习生成能够获得奖励模型高分的回答,不断提高自己的表现。

  经过多轮迭代优化后,我们最终得到一个性能优异的**ChatGPT模型**,它能够理解人类的自然语言查询,并给出高质量的回复。

以上就是对ChatGPT训练流程的简要概述。总的来说,ChatGPT的训练过程融合了自监督学习、监督学习和强化学习等多种机器学习范式,通过大规模语料的预训练、指令数据的微调、人类偏好的奖励建模和强化学习优化,使其不断接近人类理想的对话助手.

|                  阶段                   |             数据类型             |                训练目标                |                   训练方法                   |                  模型输出                  |
| :-------------------------------------: | :------------------------------: | :------------------------------------: | :------------------------------------------: | :----------------------------------------: |
|        预训练<br>[Pre-Training]         |       无标签的海量文本数据       |      学习语言的基本结构和语义信息      | 自监督学习<br>(如下一个词预测, 掩码语言模型) |          基础模型<br>[Base Model]          |
|    指令微调<br>[Instruction Tuning]     |      带有指令标签的问答数据      | 理解和执行人类指令, 生成符合期望的回答 |        监督学习<br>(基于指令标签微调)        |          对话模型<br>[Chat Model]          |
| 奖励模型训练<br>[Reward Model Training] | 人类对模型生成答案的偏好标注数据 |           学习评估答案的优劣           |      监督学习<br>(基于偏好标注数据训练)      |         奖励模型<br>[Reward Model]         |
|  强化学习<br>[Reinforcement Learning]   |  奖励模型的评估结果作为奖励信号  |   生成更加高质量和符合人类偏好的答案   |        强化学习<br>(基于奖励信号优化)        | 优化后的对话模型<br>[Optimized Chat Model] |

> |     阶段     | 数据来源                                             | 数据量(Tokens) | 模型参数量       | GPU资源使用                |
> | :----------: | :--------------------------------------------------- | :------------- | :--------------- | :------------------------- |
> |    预训练    | - 网络爬取的文本数据<br>- 书籍、文章等高质量文本数据 | 100B ~ 1T      | 1B ~ 1000B       | 1000张A100 GPU, 50 ~ 200天 |
> |   指令微调   | - 众包平台收集的问答数据<br>- 人工标注的指令数据     | 10M ~ 100M     | 与预训练阶段相同 | 10张A100 GPU, 5 ~ 20天     |
> | 奖励模型训练 | - 众包平台收集偏好标注数据<br>- 人工标注的偏好数据   | 1M ~ 10M       | 100M ~ 1B        | 10张A100 GPU, 2 ~ 10天     |
> |   强化学习   | 奖励模型生成的奖励信号                               | 动态生成       | 与预训练阶段相同 | 20张A100 GPU, 10 ~ 50天    |
>
> 以上 是 Claude 3 生成的 关于训练参数量, 训练数据量, 及GPU资源的描述, 仅作参考. 
>
> 从中我们可以知道一些大概的情况, 即预训练时会达到万亿(T)为单位的预训练数据量, 参数量的话同样会达到千/万亿的参数量. 大规模文本预训练模型通常参数量与数据量(Token)比较对应
>
> 使用的A100资源至少也得是成百上千张, 训练时间一个月左右. 

## STEP 1 预训练微调阶段

### 1 概述

预训练微调[Pretraining Fine-tuning], 也称为继续预训练[Continued Pretraining]或领域适应预训练[Domain-Adaptive Pretraining], 是一种在预训练语言模型[Pretrained Language Model, PLM]的基础上, 继续使用领域特定数据进行训练, 以便将领域知识注入到模型中的方法. 

原有的大型语言模型通常在大规模的英文语料上进行训练, 对于与英语差异较大的语言(如中文)的支持可能不够理想. 因此, 需要通过预训练微调来实现语言知识的迁移和适配. 此外, 大多数预训练语言模型是在公开的通用领域数据集上训练的, 尽管它们已经学习了语言的基本结构和语义信息, 但仍然缺乏特定领域的专业知识. 为了提高预训练语言模型在下游任务中的表现, 我们需要将这些专业领域的知识"注入"到原有的预训练模型中.

具体而言, 预训练微调的方法可以分为以下两大类:

1. 全参数微调[Full Parameter Fine-tuning]: 

   在预训练语言模型的所有参数上进行微调. 

   典型的全参数微调方法包括. 

   * 领域数据 continue pre-train -> 使用领域文本进行自回归训练
   * 使用SFT数据进行"指令式无监督学习",  -> 使用SFT 数据 进行 自回归训练

2. 部分参数微调[Parameter-Efficient Fine-tuning]: 

   只对预训练语言模型的部分参数进行微调, 或在模型中添加额外的可训练模块. 代表性的部分参数微调方法包括LoRA[Low-Rank Adaptation]、Adapter、Prefix-tuning、P-tuning、Prompt-tuning和Freeze-tuning等.

通过预训练微调, 我们可以在保留预训练语言模型已经学习到的通用语言知识的同时, 进一步融入特定领域的知识, 从而提高模型在下游任务中的性能表现. 这一过程有助于缓解预训练语言模型在面对专业领域任务时的知识缺口, 使其能够更好地理解和处理特定领域的文本数据.

### 2 标记化器

在进行详细的SFT(Supervised Fine-Tuning)[监督微调]等预训练微调介绍之前,先来讲一下什么是Tokenizer[标记化器],以及如何进行Tokenizer的训练。

通俗来讲,Tokenizer的目的就是将一句话进行切分,并将切分好的Token[标记]列表输入给模型进行训练。Tokenizer在自然语言处理(Natural Language Processing, NLP)中扮演着非常重要的角色,它能够将原始的文本数据转化为模型可以理解和处理的格式。Word Piece 分词和 Byte-level BPE 分词就是这样两种广泛使用的子词分割算法,旨在平衡词汇表大小和模型性能。

#### 2.1 Word Piece 与 Byte-level BPE

Word Piece 分词由 Google 提出,常用于 BERT 等模型的预处理步骤。该算法基于统计语言模型,以贪心的方式将单词分解为子词。初始化词汇表包含所有字符,然后不断添加能够最大程度提高语言模型似然度的子词到词汇表,直到达到预设的词汇表大小。例如,"unwanted" 可能被分解为 "_un", "want", "ed"。所有子词以下划线开头,不以下划线开头的都视为独立词。Word Piece 分词解决了未登录词(OOV)问题,提高了语言模型性能。

Byte-level BPE 分词由 OpenAI 提出,用于 GPT-2 等模型。与 Word Piece 不同,它将文本编码为字节序列(bytes),而不是 Unicode 字符。这使其能够支持多语言处理。BPE(byte pair encoding)本质上是一种数据压缩算法,将频繁出现的字节对替换为未使用的字节。当用于子词分割时,算法不断合并频率最高的相邻字节对,直到达到预设的词汇表大小。例如,"lower" 可能被编码为 "l o w er"。随着 BPE 步骤的进行,"er" 可能合并成单个 token。字节编码使其能够处理任何 Unicode 字符,不受字符集限制。Byte-level BPE 能够找到语言中自然的子词单元,在多语言任务上表现出色。

#### 2.2 分词结果数值化

在 Word Piece 或 Byte-level BPE 分词完成后,需要将分词结果转化为数值形式,以便送入神经网络模型进行训练。这个过程通常包括以下几个步骤:

首先,需要构建词汇表。对训练语料进行分词,得到一个子词列表。然后统计每个子词的频次,按照频次从高到低排序。选择频次最高的 N 个子词作为词汇表,N 的大小根据任务需求和计算资源决定。在词汇表中添加一些特殊 token,如 "[PAD]"(用于填充),"[UNK]"(用于未登录词),"[CLS]"(用于分类任务),"[SEP]"(用于分隔句子)等。

接下来,将子词映射为索引。创建一个字典,将词汇表中的每个子词映射为一个唯一的整数索引。例如,可以将词汇表中第一个子词映射为 0,第二个映射为 1,以此类推。对于词汇表外的子词,映射为 "[UNK]" 对应的索引。

然后,将文本转化为索引序列。对输入的文本进行分词,得到一个子词列表。使用上一步创建的字典,将每个子词替换为对应的索引。得到一个整数索引序列,代表原始文本。

最后,需要对输入序列进行处理。根据模型的要求,对索引序列进行截断或填充,使其长度符合模型输入的固定大小。对于 BERT 等双向语言模型,需要在序列的开头和结尾分别添加 "[CLS]" 和 "[SEP]" 对应的索引。对于 GPT 等单向语言模型,通常只需要在序列的结尾添加一个结束符。

#### 2.3 **词表扩充**

为了降低模型的训练难度,研究者通常会考虑在原来的词表上进行「词表扩充」,也就是将一些常见的汉字Token手动添加到原来的Tokenizer中,从而降低模型的训练难度。

在 BELLE(BlEnding Longue for Language Models Enhancement)[使用长尾数据增强语言模型] 中也采用了类似的做法:首先在120万行中文文本上训练出一个5万规模的Token集合,并将这部分Token集合与原来的LLaMA(Language Long-range Memory Attention)词表进行合并,最后再在3.2B(Billion)[十亿]的中文语料上对这部分新扩展的Token Embedding[嵌入]做二次预训练。

总的来说,Tokenizer在LLM的训练过程中起着至关重要的作用。通过选择合适的Tokenizer方法以及进行必要的优化(如词表扩充),可以有效提高模型的训练效率和性能,让LLM更好地理解和生成自然语言。

> **Question:**
> 在 Tokenizer 训练完毕后,如果我们加入新的 Token,会改变原有文本的概率分布吗?这对模型的性能有何影响?应该如何处理这种情况?
>
> **Answer:**
> 在 Tokenizer 训练完毕后,如果直接将新的 Token 加入词表,确实会改变原有文本的概率分布。这是因为 Tokenizer 的训练过程通常基于大量文本数据,采用贪心策略或其他优化算法来构建子词词表,目的是在控制词表大小的同时,最大化语料的覆盖率和表示能力。新加入的 Token 会影响原有 Token 的频次统计和排序,从而改变文本的概率分布。
>
> 然而,我们需要明确 **Tokenizer 的主要作用是将文本转化为模型可以处理的数值形式**,**而语义信息的学习主要发生在 Embedding 层和之后的神经网络层**。因此,在处理新加入 Token 的情况时,有两种常见的方式:
>
> 1. 重新训练 Tokenizer:将新的文本数据与原有数据合并,重新进行 Tokenizer 的训练。这样可以得到一个新的 Tokenizer,能够更好地适应新的数据分布。但是,这种方式可能会改变原有 Token 的编码,影响已经训练好的模型。
>
> 2. 冻结原有 Tokenizer:保持原有 Tokenizer 不变,将新的 Token 作为未登录词(Unknown Token)处理。在训练过程中,只更新新 Token 对应的 Embedding,而不改变原有 Token 的 Embedding。这种方式可以在不影响原有模型的情况下,适应新的数据。
>
> 如果采用第二种方式,即冻结原有 Tokenizer 并只训练新 Token 的 Embedding,就可以在保留原有语义信息的同时,学习新 Token 的语义表示。新 Token 与原有 Token 之间的语义关系和交互,主要通过 Transformer 的自注意力机制和前向/后向传播来建立。在训练过程中,模型会自动学习新 Token 与原有 Token 之间的关系,而不仅仅依赖于 Tokenizer 的分词结果。
>
> 因此,在 Tokenizer 训练完毕后加入新 Token 的情况下,采用冻结原有 Tokenizer 并训练新 Token Embedding 的方式,可以在不影响原有模型的情况下,适应新的数据并学习新 Token 的语义表示,从而维持模型的性能。

### 3 训练数据

大型语言模型[Large Language Model, LLM]的预训练[Pretraining]过程的核心思路是向模型输入大量文本数据，并训练模型完成下一个词的预测任务[Next Token Prediction]。这个过程相对容易理解。

接下来，我们将重点讨论训练数据方面的几个关键问题, 包括 训练数据来源 与 样本质量, 样本投喂。

#### 3.1 训练数据来源

构建LLM的训练数据主要来自两大类来源：**开源数据集**和**私有数据集**。

在开源数据集方面，一些著名的数据集如"Common Crawl"和"Wikipedia"被广泛应用于LLM的预训练阶段。

然而，仅仅依赖开源数据集可能无法满足构建高性能LLM的需求。为了进一步提升模型性能，一些研究机构和企业会选择使用私有数据集。私有数据集通常需要通过购买或定制化爬取等方式获得，其内容可以针对特定领域或任务进行优化。例如，为了提升LLM在医疗领域的性能，可以使用医学论文、病例报告等私有医疗文本数据进行训练。

近年来，随着LLM技术的发展，使用现有的LLM（如GPT-3）来生成合成数据[Synthetic Data]也成为了一种行之有效的数据获取途径。这种方法利用预训练好的LLM在给定prompt的情况下生成大量相关文本，从而快速构建针对特定任务的训练数据集。这种数据合成方法可以显著降低数据获取的成本，同时提高数据的针对性。但需要注意的是，使用合成数据训练LLM可能引入一些偏差和不确定性，需要在使用时进行仔细评估和权衡。

##### 3.1.1 GPT 训练数据集

在GPT-3[Generative Pre-trained Transformer 3]的训练过程中，研究者使用了多个不同的数据源。

根据OpenAI公司发表的论文，他们对不同质量的数据源采用了不同的采样比例策略：

![image-20240313092353510](assets\image-20240313092353510.png)

通常情况下，高质量的数据集规模相对较小，而低质量的数据集数据量却非常庞大。为了在训练过程中更好地利用有限的计算资源，研究者采取了一种分层采样[Hierarchical Sampling]的方法。该方法对高质量数据集的数据采样频率更高，以期最大化模型从高质量数据中学习到的知识和语言能力。同时，低质量但数据量大的数据集以相对较低的采样频率参与训练，以期帮助模型学习到更广泛的语言现象和知识。这种分层采样的策略在GPT-3的训练中被证明是非常有效的。

需要指出的是，在构建LLM的训练数据时，数据的质量和数量之间往往存在一个权衡。

高质量的数据有助于模型学习到更准确、更连贯的语言表达能力，但这类数据的规模往往有限。反之，互联网上存在大量的低质量文本数据，如果加以妥善清洗和过滤，其规模优势也可以在一定程度上弥补质量上的不足，帮助模型学习到更鲁棒、更全面的语言能力。因此，如何在数据质量和数量之间进行平衡，是构建LLM训练语料库需要仔细权衡的问题。

##### 3.1.2 中文开源数据集

比较著名的开源中文数据集有 [天工 150B](https://github.com/SkyworkAI/Skywork) 数据集. 

> [天工Skywork-13B开源模型的炼成和思考 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/664985891)
>
> 国内 : [始智AI-wisemodel-中国AI开源创新社区](https://www.wisemodel.cn/datasets/Skywork/SkyPile-150B/)
>
> 国际 : https://huggingface.co/datasets/Skywork/SkyPile-150B

中文预训练数据集 [[悟道](https://link.zhihu.com/?target=https%3A//data.baai.ac.cn/details/WuDaoCorporaText)]，数据集分布如下（主要以百科、博客为主）：

![image-20240315175832880](assets\image-20240315175832880.png)

除此之外, [这里 ](https://blog.csdn.net/OpenBayes/article/details/135859340)还有20个LLM的数据集

##### 3.1.3 基于PHI的合成数据集构建

对于规模较小的公司或开源团队而言，从头构建数据清洗和处理流程可能面临诸多挑战，如人力和经费有限、缺乏相关经验等。在这种情况下，利用预训练的大型语言模型[Pre-trained Large Language Model, PLM]（如GPT-3）来生成合成数据是一种经济高效的替代方案。

具体而言，可以采用以下流程来构建合成数据集：

1. 对现有的开源数据集进行聚类[Clustering]，将语义相似的文本划分到同一个主题[Topic]中。

   常用的文本聚类算法包括K-Means、DBSCAN等。

2. 从每个主题中提取关键词、短语以及典型句式，构建针对该主题的提示[Prompt]模板。

3. 将提示模板输入预训练的PLM中，利用其强大的语言生成能力来合成大量与该主题相关的文本数据。在生成过程中，可以通过调整采样策略（如Top-k采样、Nucleus采样等）来控制生成文本的多样性和质量。

4. 对生成的合成文本进行后处理，如过滤、去重等，以进一步提高数据质量。

通过上述流程，即使是资源有限的小团队，也能够利用PLM的能力快速构建大规模、高质量的合成数据集。这种数据合成方案不仅成本较低，而且可以根据具体任务的需求来定制化生成数据，具有很大的灵活性。

需要注意的是，使用合成数据训练LLM可能引入一些偏差和不确定性。合成数据的质量和多样性在很大程度上取决于PLM的性能以及prompt设计的合理性。因此，在使用合成数据时，需要对其质量进行仔细评估，并与真实数据集进行对比，以确保模型能够学习到有效且无偏的语言知识。此外，还需要关注合成数据在隐私保护和知识产权方面的潜在风险，确保数据合成过程的合规性。

#### 3.2 训练样本质量控制

在大规模语言模型[Large Language Model, LLM]的预训练[Pre-training]阶段，训练数据的质量对模型的性能和泛化能力有着至关重要的影响。为了获得高质量的预训练语料，业界通常会采用一系列数据清洗和过滤的方法，分为基础清洗和进阶清洗两个层次。

##### 3.2.1 基础清洗

基础清洗主要针对一些明显的数据质量问题进行处理，旨在提高训练样本的整体质量，减少噪声对模型训练的影响。在构建大规模语言模型[Large Language Model, LLM]的过程中，数据清洗是一个至关重要的步骤。高质量的训练数据能够显著提升模型的性能和泛化能力。以下是一个典型的LLM数据清洗流程，基础清洗包括七个主要步骤：

1. 语言识别：首先，需要使用语言识别模型对爬取的网页数据进行筛选，过滤掉非目标语言的文本，确保训练样本的语言一致性。这一步通常在数据爬取之后、其他清洗步骤之前进行。

2. URL过滤：在获得目标语言的网页数据后，需要根据预定义的URL黑名单（如成人网站）和URL分数来决定是否保留该网页。URL黑名单可以基于域名关键词、正则表达式等方式构建，而URL分数则可以通过机器学习模型来预测网页的质量和相关性。

3. 内容抽取：对于保留下来的URL，我们需要从网页中提取出主要的文本信息，并过滤掉无关内容，如导航栏、广告等。这一步可以使用现有的开源工具，如trafilatura库，它支持通过指定HTML元素来过滤不需要的内容。

4. 篇章级别过滤：在获得纯文本数据后，需要进一步进行篇章级别的过滤。主要策略包括：
   1. a)去除内容重复的文章；

   2. b)根据文章长度、标点符号比例等指标，过滤掉质量低下的文章。

5. 句子级别过滤：在篇章过滤的基础上，还需要对文章中的每个句子进行筛选，去除一些无意义、话术繁多但缺乏实际内容的句子。这类句子通常难以穷举，需要采用规则或机器学习的方法进行识别。

6. 去除导致困惑度[Perplexity, PPL]异常的样本：一些过短、过长、重复、乱码或无意义的文本可能会导致语言模型的PPL值异常，需要将其过滤掉。PPL值反映了语言模型对给定文本的"惊讶程度"，异常的PPL值通常意味着文本质量较差。

7. 过滤政治敏感内容：为了避免模型学习到有偏见或争议性的内容，需要使用关键词匹配、正则表达式等方法，过滤掉政治敏感文本。此外，还需要注意删除重复数据，以防止模型过拟合。

以上七个步骤构成了一个完整的LLM数据清洗流水线[Pipeline]。为了提高效率，可以利用Apache Beam等分布式数据处理框架实现自动化。此外，一些高质量的开源数据集如<天工>，已经在很大程度上满足了数据清洗的要求，可以作为LLM训练的优质数据来源，减少数据清洗的工作量。

> From Falcon
>
> <img src="assets\image-20240315180119107.png" alt="image-20240315180119107" style="zoom: 25%;" /> 
>
> <img src="assets\v2-71de5e4621aaa7348164930439a0dcd8_r.jpg" alt="img" style="zoom: 26%;" /> 

##### 3.2.2 进阶清洗

在完成基础清洗后，引入"标签[Label]"的概念可以进一步提升数据质量，这是一种行之有效的进阶清洗策略。

"标签"是指用于描述和评估数据质量的各种属性或指标，可以从不同维度刻画文本数据的特点，如语言种类、主题领域、质量评分、重复度、噪声水平、长度分布、来源渠道、时间戳等。通过为每个数据样本赋予一个或多个标签，我们可以更全面地把握数据的整体面貌，并针对性地开展清洗和优化工作。

标签的生成方式多种多样，可以借助启发式规则、正则表达式、机器学习模型等技术手段。例如，使用文本分类模型[Text Classification Model]对文章的主题进行标注，使用语言检测工具[Language Detection Tool]识别文本的语种，使用质量评估模型[Quality Assessment Model]对文章的可读性和流畅性打分，使用重复检测算法[Duplication Detection Algorithm]标识近似重复的内容，使用模式匹配方法[Pattern Matching Method]标记特定类型的噪声（如广告、脚本等）。

在获得丰富的标签信息后，数据清洗可以通过灵活组合和应用这些标签来实现。常见的清洗操作包括：根据标签阈值过滤不合要求的样本，根据标签分布对不同类别的数据进行采样平衡，基于重复标签去除冗余数据，依据长度标签对过长文本进行截断，结合多个标签对数据进行细粒度的筛选和排序等。通过这些操作，可以有效地提升预训练语料的质量和多样性。

需要注意的是，数据清洗是一个动态迭代的过程。随着标签体系的不断完善和优化目标的推进，清洗的策略和流程也会变得越来越复杂和精细。不同阶段可能会引入新的标签维度，对已有标签的定义和生成方法进行调整，探索标签之间的相关性和组合效果。同时，也要权衡标签的覆盖面和标注成本，选择性地投入资源优化关键标签。

总之，基于标签的进阶数据清洗是LLM预训练的重要环节，它为构建高质量的训练语料提供了有力的工具和方法。通过对标签的深入研究和应用，我们可以从多个维度评估和改进数据质量，为训练出更加强大和鲁棒的语言模型奠定基础。同时，这也为其他类型的机器学习任务的数据优化提供了有益的思路和借鉴。

#### 3.3 分阶段样本添加

在大语言模型[Large Language Model, LLM]的预训练过程中,训练样本的选择与组合策略对模型性能有着重要影响。通过分阶段调整训练样本的质量与分布,可以在不同训练阶段实现不同的优化目标,提升模型的收敛速度、泛化能力与领域表现。以下是三种常见的分阶段样本添加策略:

1. 末期高质量样本策略 
该策略在模型训练的快速收敛阶段[Fast Convergence Stage]和平稳阶段[Stable Stage]均使用普通样本进行训练,在退火阶段[Annealing Stage]额外混入高质量样本。通过在训练后期引入高质量样本,模型可以在普通样本的基础上进一步提升特定领域的表现,实现"教科书式"的学习。这种方式有助于在保证模型泛化能力的同时,提升其在特定领域的性能表现。

2. 初期高质量样本策略
该策略在快速收敛阶段以高质量样本为主,帮助模型快速收敛至较好的初始状态。在平稳阶段,逐步增加普通样本的比例,让模型在保持高质量样本学习效果的同时,逐步适应更加多样化的语料。退火阶段的样本分布与平稳阶段保持一致。这种方式的优势在于利用高质量样本引导模型快速收敛,但可能面临以下问题:
   - 高质量样本数量有限,重复学习可能导致过拟合。但支持者认为,适度的重复学习,特别是对于小模型而言,并不会带来显著的负面影响。
   - 初期高质量样本可能使模型局限于特定领域或语料的特征,影响后续在普通样本上的泛化表现。但支持者认为,与末期高质量样本策略相比,该策略反而更有利于模型的泛化能力。

3. 全程高质量样本策略
   该策略全程仅使用高质量样本进行训练,代表性的工作如PHIL(Projected Heads in Layers)。这是在特定领域取得优异表现的有效方法之一。
   - 优势:在特定任务或领域上,模型的性能表现会非常突出。
   - 劣势:模型的泛化能力可能较差。但PHIL本身的目标就是探索小模型在特定领域达到SOTA的可能性,而非训练一个"万能"的世界模型。

分阶段样本添加策略是LLM训练的重要环节。

通过在不同阶段灵活调整样本质量与分布,可以兼顾模型的泛化能力与领域表现,进而实现特定的优化目标。不同策略之间的权衡取舍,需要结合具体的任务需求与模型特点进行选择。

### 4 模型优化

> Flash Attention [[falcon](https://link.zhihu.com/?target=https%3A//huggingface.co/tiiuae/falcon-40b)]
>
> ALiBi（[[Bloom](https://link.zhihu.com/?target=https%3A//huggingface.co/bigscience/bloom-7b1)])
>
> RoPE（[[GLM-130B](https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/THUDM/GLM-130B)]

#### 4.1 注意力机制加速

为了加快大型语言模型[Large Language Model, LLM]的训练速度，研究者通常会在解码器[Decoder]模型中加入一些技巧来缩短模型训练周期。

注意力机制[Attention Mechanism]是Transformer架构语言模型的核心组件，它允许模型在处理当前位置时参考输入序列的其他部分，从而捕捉长距离依赖关系。然而，注意力计算也是这些模型训练和推理过程中的主要性能瓶颈。根据多个研究报告，注意力计算通常占据了语言模型训练和推理时间的50%至80%，具体比例取决于模型配置、数据和硬件环境等因素。

> ① 根据NVIDIA的报告，在使用GPT-3模型进行推理时，注意力计算占据了整个前向过程的约60%的时间。这个比例在训练过程中可能更高，因为训练还涉及反向传播和梯度计算。
>
> ② 在谷歌的一项研究中，研究者发现在BERT-Base模型的训练过程中，注意力计算占据了总训练时间的约30%到50%，具体比例取决于所使用的硬件和批次大小[Batch Size]。
>
> ③ 微软的一项研究显示，在使用Transformer-XL模型进行语言建模任务时，注意力计算占据了训练过程的70%以上的时间。
>
> ④ 在OpenAI的GPT-2模型中，研究者报告称注意力计算占据了训练过程的约80%的时间。

注意力机制之所以如此耗时，主要源于以下几个原因：

首先，注意力计算涉及大量的**矩阵运算**，尤其是在计算查询[Query]、键[Key]、值[Value]矩阵之间的相似度时，需要执行大规模的矩阵乘法操作。对于序列长度为$n$，隐藏层维度为$d$的输入，标准的缩放点积注意力[Scaled Dot-Product Attention]的计算复杂度为$O(n^2d)$，这在处理长序列时会导致巨大的计算开销。

其次，注意力计算涉及**大量的内存读写操作**，特别是在计算键值矩阵时，需要在大规模的参数矩阵中进行随机访问。这种不连续的内存访问模式会导致频繁的缓存未命中，从而降低了计算效率。此外，注意力矩阵的大小通常与序列长度的平方成正比，这意味着在处理长序列时，注意力机制会消耗大量的内存带宽。

再者，尽管现代GPU擅长并行计算，但注意力机制的某些部分**难以高效地并行化**。例如，在计算软最大值[Softmax]时，需要对注意力分数进行归一化，这涉及全局信息的通信和同步，限制了并行性。同样，在计算注意力输出时，需要将值矩阵与注意力权重进行加权求和，这也涉及跨不同位置的信息通信。

最后，随着语言模型规模的不断增长，注意力机制中的参数矩阵也变得越来越大。以GPT-3为例，其最大版本使用了96个注意力头，每个头的维度达到了12,288。如此巨大的参数规模进一步放大了注意力计算中的计算和内存开销。

为了缓解这些问题，研究者提出了各种注意力机制的变体和优化技术。例如，稀疏注意力[Sparse Attention]通过限制每个位置只关注一部分上下文，降低了计算复杂度；局部敏感哈希注意力[Locality-Sensitive Hashing Attention, LSH Attention]通过哈希函数将相似的位置聚合在一起，提高了内存访问的局部性。此外，一些先进的注意力变体，如多查询注意力[Multi-Query Attention, MQA] 和线性偏置注意力[Attention with Linear Biases, ALiBi]，通过**参数共享**和**相对位置编码**等技术，在保持模型性能的同时显著减少了计算和内存开销。

> MQA通过参数共享减少了参数量，它让所有的注意力头[Attention Head]共享同一份键[Key]矩阵和值[Value]矩阵，每个头只单独保留一份查询[Query]参数，从而大大减少了键值矩阵的参数量. 这一技巧最早出现在2019年，并被应用于Falcon-40B等模型中。

除了算法层面的优化，**硬件加速技术**也在注意力机制的加速中发挥了重要作用。例如，针对注意力计算优化的Tensor Core，通过定制的指令和数据流提高了计算效率。混合精度[Mixed Precision]训练技术通过动态调整数值精度，在降低内存占用的同时加快了计算速度。

#### 4.2 不同样本长度下的优化

在大型语言模型的训练和应用中，样本长度的差异常会对模型性能产生显著影响。传统的Transformer架构在处理不同长度的序列时存在以下挑战：

首先，Transformer在训练时通常需要将所有样本都padding到一个固定的最大长度，这会导致计算和内存资源的浪费，尤其是在处理长度差异较大的样本时。例如，如果batch中最长的样本有1000个token，而其他样本平均只有200个token，那么就会有大量的计算资源被浪费在无意义的padding上。

其次，由于位置编码[Position Embedding]的限制，Transformer难以将在短序列上学到的知识泛化到长序列上。传统的固定位置编码通过一个固定的映射函数将位置信息嵌入到输入表示中，这在训练和推理长度不一致时会导致性能下降。例如，如果模型在长度为512的序列上训练，但在推理时需要处理长度为1024的序列，那么模型的性能通常会显著下降。

为了解决这些问题，研究者提出了多种针对不同样本长度的优化技术。其中，稀疏注意力机制通过动态调整注意力范围，允许模型在不同长度的序列上都能捕捉到关键的依赖关系。例如，Longformer使用了局部窗口注意力和全局注意力的组合，在处理长文档时能够兼顾局部上下文和全局信息。而Sparse Transformer则引入了基于Hash的注意力机制，通过随机Hash函数将tokens分配到不同的buckets中，在每个bucket内部执行全局注意力，从而在保持计算效率的同时捕捉长距离依赖。

在位置编码方面，相对位置编码[Relative Position Embedding]被证明是一种有效的解决方案。不同于传统的固定位置编码，相对位置编码考虑的是tokens之间的相对位置关系，因此能够更好地适应不同长度的序列。例如，Transformer-XL使用了基于乘法的相对位置编码，通过在注意力计算中引入相对位置偏置项，使得模型能够在任意长度的序列上进行高效的计算。类似地，T5模型使用了基于正弦曲线的相对位置编码，通过周期性函数对相对位置进行建模，提高了模型在不同长度序列上的泛化能力。

此外，为了让模型能够在不同长度的样本上都具备较好的推理能力，通常也会在位置编码上进行一些处理，如选用线**性偏置注意力**[Attention with Linear Biases, ALiBi] (2022)或**旋转位置编码**[Rotary Position Embedding, RoPE]等方法。ALiBi通过相对位置编码提高了训练效率，解决了Transformer在训练和推理时文本长度不一致的难题。论文中在训练时使用1024的最大长度，但在推理时用2048的最大长度，并且在困惑度[Perplexity, PPL]指标上取得了与训练长度相当的结果。这一方法被应用于BLOOM等模型中。RoPE则被GLM-130B等模型采用。

除了上述方法，一些**动态计算技术**也被用于处理不同长度的样本。例如，Universally Sparse Transformer引入了一种自适应的稀疏注意力模式，根据输入的长度和内容动态调整注意力的稀疏程度，在降低计算复杂度的同时提高了模型的表示能力。而DynamicConv则使用了动态卷积核来替代自注意力，通过在不同位置使用不同的卷积参数，实现了对不同长度序列的高效处理。

这些加速方法主要适用于大型语言模型的预训练[Pre-training]阶段，在此阶段模型通常在较短的句子上训练，而在较长的句子上进行推理。通过合理运用这些技巧，研究者能够显著提升模型的训练速度和推理效率，促进模型性能的进一步提高。

综上所述，大型语言模型在处理不同长度样本时面临着计算效率和泛化能力的双重挑战。稀疏注意力、相对位置编码、动态计算等技术的出现，为解决这一问题提供了新的思路。通过在注意力机制和位置编码中引入更灵活的计算方式，这些方法能够帮助语言模型在不同长度的序列上都取得较好的性能，提高了模型的适用性和鲁棒性。未来，随着语言模型规模的不断扩大，样本长度差异带来的挑战也会越来越突出。这需要研究者在算法、架构、硬件等多个层面进行持续的创新，以充分发挥大型语言模型在自然语言处理中的潜力。

### 5 模型训练

####  5.1 预训练过程中的分阶段策略

大规模语言模型(Large Language Models, LLMs)[1]的预训练过程通常采用分阶段的策略,以ChatGPT[2]为代表的LLMs预训练主要包括以下三个阶段:快速收敛阶段(Rapid Convergence Stage)、稳定阶段(Stable Stage)和退火阶段(Annealing Stage)[3]。值得一提的是,MiniCPM[4]首次明确提出了退火阶段的概念。

上述分阶段训练策略的提出主要源于研究者对LLMs训练过程中损失函数(Loss Function)曲线的观察。损失函数收敛曲线往往呈现出明显的阶段性特点,不同阶段的曲线形态各异。针对不同Loss曲线阶段,研究者通过调整训练样本和超参数,能够获得更优的模型性能。

近期,业界流行一种做法:在预训练的最后阶段补充一些与下游任务相似的数据,以提升模型在特定任务上的表现。这种做法俗称"和面——水多了加面,面多了加水"。起初,这种做法饱受质疑,被认为是一种"刷榜"行为。然而,MiniCPM更进一步,直接将监督微调(Supervised Fine-Tuning, SFT)数据混入预训练数据中,更加直接地影响模型性能。

对于上述做法,我们可以从以下两个角度理解其合理性:

1. 从模型学习的训练动态角度看,在退火阶段,Loss下降速度较稳定阶段和正常的余弦退火(Cosine Annealing)更快,表明此时模型的学习效率处于较高水平。在这一阶段使用高质量数据训练模型,可以最大限度地发挥数据的价值。

2. 与普通文本数据相比,SFT数据更偏向于基准测试(Benchmark),因为SFT数据大多采用"问答"(Question-Answering, QA)结构,有助于提升模型在基准测试中的表现。

传统做法中,预训练完成后直接使用SFT数据进行微调,语料分布差异较大,这在本质上是不理想的。而将SFT数据适当混入预训练,则有助于保证后续通用领域微调的一致性,前提是SFT数据的比例要适中。

总的来说,对于有能力从零开始训练大模型的公司而言,训练流程通常为:训练数据整理 -> 分阶段训练(在训练末期添加SFT数据)。而对于没有足够计算资源的公司,或仅需要特定领域LLMs的公司,则经历领域知识微调(Domain Adaptation) -> 监督微调(Supervised Fine-Tuning)的过程。

#### 5.2 Warmup & LR Scheduler 设置

在LLM的预训练阶段,合理设置学习率调度(Learning Rate Scheduler)和优化器至关重要。学习率决定了模型参数更新的步长,而优化器则决定了参数更新的方向和策略。二者需要协同工作,以实现模型的高效训练和收敛。

通常,我们会在训练的初期采用预热(Warmup)策略,即先使用较小的学习率,然后在预热阶段逐渐增大学习率。这样做的目的是避免模型在训练初期就陷入局部最优,同时也有助于模型稳定地适应训练数据。预热阶段结束后,学习率通常会采用余弦退火(Cosine Annealing)的方式进行调整,即学习率会以余弦函数的周期性变化。这种学习率调度策略被称为 Cosine Annealing with Warmup。

在选择具体的学习率大小和预热步数时,需要综合考虑训练资源的充足程度。

* 当训练资源充足时,我们可以选择较大的学习率,以便模型更好地适配下游任务。
* 当资源有限时,使用较小的学习率和更长的预热步数可能是更好的选择,以避免模型过拟合和训练不稳定。

> 对于不同规模的 Base LLM 开源模型,在进行继续预训练任务时,学习率的选择确实需要根据模型规模和训练资源等因素进行调整。以下是一些常见的学习率范围和预热步数设置:
>
> 1. 对于较小规模的 Base LLM(如 BERT-Base、GPT-2 等),学习率通常在 1e-5 到 5e-5 之间。例如:
>    - BERT-Base 在继续预训练时,学习率通常设置为 2e-5 到 3e-5。
>    - GPT-2 在继续预训练时,学习率通常设置为 1e-5 到 2e-5。
>
>    对于这些较小规模的模型,预热步数通常设置为 1000 到 10000 步不等,具体取决于训练数据的大小和训练的总步数。
>
> 2. 对于中等规模的 Base LLM(如 RoBERTa-Large、GPT-Neo 等),学习率通常在 5e-6 到 2e-5 之间。例如:
>    - RoBERTa-Large 在继续预训练时,学习率通常设置为 1e-5 到 1.5e-5。
>    - GPT-Neo 在继续预训练时,学习率通常设置为 5e-6 到 1e-5。
>
>    对于这些中等规模的模型,预热步数通常设置为 5000 到 20000 步不等,同样取决于训练数据的大小和训练的总步数。
>
> 3. 对于大规模的 Base LLM(如 T5、GPT-3 等),学习率通常在 1e-6 到 1e-5 之间。例如:
>    - T5 在继续预训练时,学习率通常设置为 5e-6 到 1e-5。
>    - GPT-3 在继续预训练时,学习率通常设置为 1e-6 到 5e-6。
>
>    对于这些大规模的模型,预热步数通常设置为 10000 到 50000 步不等,甚至更多,具体取决于训练数据的大小和训练的总步数。
>
> 需要注意的是,以上学习率范围和预热步数设置仅是一般经验,实际应用中还需要根据具体任务、数据集、优化器、批次大小(Batch Size)等因素进行调整

优化器的选择也需要根据具体任务和模型架构而定。以前,计算机视觉任务常用 SGD with Momentum 优化器,而自然语言处理任务(特别是基于 Transformer 的模型)常用 Adam 优化器。但随着 Transformer 在计算机视觉中的广泛应用,AdamW 优化器逐渐成为主流选择。优化器的核心目标是加速模型学习,并帮助其跳出局部最优。现有的优化器大多基于历史梯度信息进行自适应调整,如 Momentum、AdaGrad 和 Adam 等。

需要注意的是,优化器本质上是基于经验和假设而设计的规则,难以完美适配所有任务。因此,我们还需要学习率调度来辅助优化器工作。一个值得尝试的思路是将 Cosine Annealing 与多个周期相结合,即在训练的大部分阶段采用多个余弦周期,这可能有助于模型更好地跳出局部最优。在训练即将结束时,可以将学习率升高,进入稳定期(Stable Phase)和衰减期(Decay Phase),类似于 Warm-up 阶段的逆过程。这种改进后的学习率调度策略可以称为 Warm-Cosine-Stable-Decay (WCSD) 策略。

总之,LLM 的预训练是一个复杂的过程,需要在学习率调度、优化器选择、训练策略等多个方面进行精细的设计和调整,以达到最佳的训练效果。这需要研究者和工程师在理论研究和实践经验之间进行反复探索和权衡。

#### 5.3 Batch Size

在大规模语言模型[Large Language Model, LLM]的预训练阶段,批量大小[Batch Size]的选择是一个重要的超参数。研究人员发现,批量大小存在一个权衡[trade-off]。当批量大小较小时,训练过程中的损失函数波动较大,模型不易收敛。然而,这种波动也有助于模型跳出局部最优,从而获得更好的泛化能力。相反,当批量大小较大时,训练的步数会相应减少,每一步的梯度更新也会更加平稳。在相同的训练样本数量下,较大的批量大小可能导致模型收敛速度变慢。

为了在使用大批量大小时兼顾训练效率和模型性能,研究人员探索了各种优化方法。2020年,有研究者专门研究了如何在使用大批量大小的同时,既能加快训练速度,又能保证模型的收敛效果。其中一个解决思路是优化优化器[Optimizer]。例如,谷歌在当年提出的LAMB优化器,通过对自适应学习率和Layer-wise自适应率缩放[Layer-wise Adaptive Rate Scaling]的改进,成功地将批量大小从**512扩大到了60,000**,且在相同的训练时间内取得了与小批量大小相当的收敛效果。

此外,Facebook AI Research(FAIR)在2021年提出了SSNB(Stochastic Sandwich Natural Gradient with Barrier)优化器,通过引入随机三明治自然梯度[Stochastic Sandwich Natural Gradient]和障碍函数[Barrier Function],在使用大批量大小时取得了更快的收敛速度和更好的泛化性能。

总的来说,在LLM的预训练阶段,批量大小的选择需要综合考虑**训练效率和模型性能**。通过优化优化器等方法,研究人员正在不断探索如何在使用大批量大小的同时,兼顾训练速度和模型质量,以期在更短的时间内训练出性能更优的大规模语言模型。

> 大规模语言模型的预训练阶段,常用的批量大小因模型架构、训练数据集和硬件条件的不同而有所差异。以下是一些著名的大规模语言模型及其训练时使用的批量大小:
>
> 1. BERT(Bidirectional Encoder Representations from Transformers):原始的BERT模型在预训练阶段使用了批量大小为**256**的设置,在16个TPU芯片上进行了4天的训练。
>
> 2. GPT-3(Generative Pre-trained Transformer 3):OpenAI在训练GPT-3模型时,使用了批量大小为**3.2M(3,200,000)**的设置,这是迄今为止在语言模型训练中使用的最大批量大小之一。GPT-3的训练在V100 GPU集群上进行,历时几个月。
>
> 3. T5(Text-to-Text Transfer Transformer):Google在训练T5模型时,使用了批量大小为**2048**的设置,在256个TPU芯片上训练了几天时间。
>
> 4. Megatron-LM:NVIDIA与微软合作开发的Megatron-LM模型在预训练阶段使用了批量大小为**1920**的设置,在DGX SuperPOD系统上使用了1024个NVIDIA V100 GPU进行训练。
>
> 5. PaLM(Pathways Language Model):Google在训练PaLM模型时,使用了批量大小为**1024**的设置,在6144个TPU芯片上训练了几个月时间。
>

### 6 预训练模型评估

关于语言模型(Language Modeling)的评估指标,目前业界较为常用的有困惑度(Perplexity,简称PPL)、每字符比特数(Bits-per-character,简称BPC)等。这些指标本质上都是在生成结果和目标文本之间的交叉熵损失(Cross Entropy Loss)的基础上做了一些变换。具体而言,PPL是交叉熵的指数形式,而BPC则是交叉熵除以序列长度得到的平均值。

这类指标可以用来评估语言模型对于语言规律和模式的掌握程度,即在给定上文的情况下,预测后续片段中各个词Token的概率分布与真实分布的接近程度。一个训练良好的语言模型应当能够生成流畅、通顺且符合语法规范的句子。然而,随着语言模型的发展,仅仅具备"生成通顺句子"的能力已经不足以满足人们日益增长的需求。当前大部分大型语言模型(Large Language Model,简称LLM)都已具备生成流畅语句的能力,单从流畅度很难区分不同模型的优劣。

因此,我们还需要评估语言模型的另一项重要能力——**知识蕴含能力(Knowledge Embedding Capability)**,即模型在预训练过程中习得的背景知识和常识性认知。由于在预训练阶段模型还未经过指令微调(Instruction Tuning),无法实现人机友好交互,因此需要采用基于概率的方法来测试模型的知识蕴含能力。

**下面简要介绍一种经典的中文评估数据集及其使用方法。在针对特定领域训练语言模型时,我们通常需要构建一个与该领域对应的评估数据集,用以评判模型在目标领域的表现。**

总的来说,语言模型的评估需要综合考虑其语言建模能力和知识蕴含能力两个方面。只有在这两个层面都有出色表现的语言模型,才能满足实际应用需求,在开放域对话、问答、知识库问答等任务中取得优异成绩。展望未来,如何更加全面、细致地评估语言模型的性能,设计更加贴近实际需求的评估指标和数据集,将是语言模型研究领域的重要课题。

#### **C-Eval**

一个很好的中文知识能力测试数据集是 [[C-Eval](https://link.zhihu.com/?target=https%3A//github.com/SJTU-LIT/ceval)]，涵盖1.4w 道选择题，共 52 个学科。

![img](assets\v2-9eaf3f515f7a0c45a8ebc312f0c86438_720w.webp)\

由于是选择题的形式，我们可以通过将题目写进 prompt 中，并让模型续写 1 个 token，判断这个续写 token 的答案是不是正确答案即可。但大部分没有人工对齐的预训练模型可能无法续写出「A B C D」这样的选项答案，因此，官方推荐使用 5-shot 的方式来让模型知道如何输出答案：

```python
以下是中国关于会计考试的单项选择题，请选出其中的正确答案。

下列关于税法基本原则的表述中，不正确的是____。
A. 税收法定原则包括税收要件法定原则和税务合法性原则
B. 税收公平原则源于法律上的平等性原则
C. 税收效率原则包含经济效率和行政效率两个方面
D. 税务机关按法定程序依法征税，可以自由做出减征、停征或免征税款的决定
答案：D

甲公司是国内一家领先的新媒体、通信及移动增值服务公司，由于遭受世界金融危机，甲公司经济利润严重下滑，经营面临困境，但为了稳定职工队伍，公司并未进行裁员，而是实行高层管理人员减薪措施。甲公司此举采用的收缩战略方式是____。
A. 转向战略
B. 放弃战略
C. 紧缩与集中战略
D. 稳定战略
答案：C

...             # 第 3, 4, 5 道样例题

下列各项中，不能增加企业核心竞争力的是____。
A. 产品差异化
B. 购买生产专利权
C. 创新生产技术
D. 聘用生产外包商
答案：
```

通过前面的样例后，模型能够知道在「答案：」后面应该输出选项字母。

于是，我们获得模型续写后的第一个 token 的概率分布（logits），并取出「A B C D」这 4 个字母的概率，通过 softmax 进行归一化：

```python
probs = (
    torch.nn.functional.softmax(
        torch.tensor(
            [
                logits[self.tokenizer.encode(
                    "A", bos=False, eos=False)[0]],
                logits[self.tokenizer.encode(
                    "B", bos=False, eos=False)[0]],
                logits[self.tokenizer.encode(
                    "C", bos=False, eos=False)[0]],
                logits[self.tokenizer.encode(
                    "D", bos=False, eos=False)[0]],
            ]
        ),
        dim=0,
    ).detach().cpu().numpy()
)
pred = {0: "A", 1: "B", 2: "C", 3: "D"}[np.argmax(probs)]       
```

### 7 经验 OR 信息

* ① 在资源充足的情况下，建议使用SFT进行全量微调。部分参数微调的方法不稳定，有的场景下效果不理想

* ② 很多工作尝试通过中文语料二次预训练, 将优秀模型英语上的优秀能力迁移到中文任务中来. 
  如[[Chinese-LLaMA-Alpaca](https://link.zhihu.com/?target=https%3A//github.com/ymcui/Chinese-LLaMA-Alpaca)]

* ③ 根据 Scaling law，模型越大，高质量数据越多，效果越好。

  但还有一个很直观的情况，随着预训练样本的质量不断提升，训练手段的优化。新的模型，**往往效果能轻松反超参数量两倍于它的模型**。这说明，**现有参数量情况下，哪怕是2B尺度，也并没有得到充分训练**。

  ( 例如，最新出的minicpm，微信内部评测效果也是非常棒的。跟规模相对接近的2b、7b模型比，得分比qwen2b高，和qwen7b比有的高有的低 )

* ④ 关于预训练步骤时中文数据的样本比例, 大家已经达成一些基础的共识, 如中英混合比例大家都大差不差. 
* ⑤ SFT 中 越大的模型，越聪明的模型，需要的SFT数据就越少. 同理，越大的模型，越聪明，复杂样本混合比例就可以越高。(复杂样本: 刻画维度有很多，需要挖掘) 对于更大和更强的模型,可以适当提高训练数据中复杂样本的比例。复杂样本可能包括长度更长、语法结构更复杂、语义更丰富的文本。大模型更有能力处理这些复杂样本, 并从中学习到更多的知识。
* ⑥ 在数据清洗中, 与SFT一样，大公司产出各种各样的label来刻画数据，有的公司实习生就优化几个label
* ⑦ **小模型样本的极限** 到底喂了多少tokens，小模型参数才算是充分得到训练？并没有一个很好的结论。
* ⑧  **warmup 的步数不会影响最终性能**, 当模型经过「充分」训练后，不管多长的预热步数最后的性能都差不多。
* ⑨ 学习率越大，下游任务越好，上游任务越差 (忘得越多), 对于学习率的范围 从 1.5 e-4 到 6e-4 之间
* ⑩ 在初始预训练中使用 Rewarmup 会损伤性能, 尽管 warmup 策略在 Finetune 和 Continue Pretraining 中都起到了更好的效果（相较于常量学习率），但是，这建立在「切换了训练数据集（数据分布）」的前提下。在原数据集上使用 warmup 接着训练会造成性能损伤，学习率越大则损伤越大

## STEP 2 指令微调阶段

### 1 概述

在完成第一阶段的预训练[Pre-training]（或继续预训练[Continued Pre-training]）后，就可以进入指令微调阶段了。

预训练任务的本质是「续写」[Continuation]，而这种方式并不一定能够很好地回答用户的问题。由于训练数据大多来自互联网，我们无法保证其中只存在规范的「一问一答」格式，这就会导致预训练模型通常无法直接给出人们想要的答案。然而，这并不意味着预训练模型「无知」，只是需要我们用一些巧妙的「提示」[Prompt]来引导出答案。不过，这种需要用户精心设计提示从而去「套取」答案的方式，显然没有那么优雅。

事实上，模型已经掌握了这些知识，只是表达方式不符合我们人类的对话习惯。因此，我们只需要再教会模型「如何对话」即可。

这就是指令微调要完成的任务，即通过有监督的学习，让模型理解人类的指令并生成相应的回复。在这个阶段，我们需要准备一个由人工标注的高质量指令-回复数据集[Instruction-Response Dataset]，并在此基础上对预训练模型进行微调[Fine-tuning]。通过这种方式，模型将学会理解人类的意图，并以自然、流畅的方式进行回复，最终实现指令对齐[Instruction Alignment]。

> #### SFT 使用策略 
>
> 在进行领域任务的 SFT 时,我们通常有以下几种训练模式可供选择。根据具体的领域任务、领域样本情况以及业务需求,我们可以选择最合适的训练模式。
>
> 1. **模式一:基于基础模型+领域任务的 SFT。** 
>
>    该模式直接在预训练得到的基础模型上,使用领域任务数据进行 SFT,是最简单直接的训练模式。
>
> 2. **模式二:基于基础模型+领域数据持续预训练+领域任务 SFT。** 
>
>    该模式先在基础模型上使用领域数据进行持续预训练[Continue Pre-training],然后再进行领域任务的 SFT。这种模式可以让模型更好地适应领域数据的分布,提高后续 SFT 的效果。
>
> 3. **模式三:基于基础模型+领域数据持续预训练+通用任务 SFT+领域任务 SFT。** 
>
>    该模式在模式二的基础上,在领域任务 SFT 之前,先使用通用任务数据进行一次 SFT。这种模式可以兼顾模型的通用能力和领域任务性能。
>
> 4. **模式四:基于基础模型+领域数据持续预训练+通用任务与领域任务混合 SFT。** 
>
>    该模式与模式三类似,但在最后一步 SFT 时,同时使用通用任务数据和领域任务数据进行混合训练,而不是分别进行。
>
> 5. **模式五:基于基础模型+领域数据持续预训练(混入 SFT 数据)+通用任务与领域任务混合 SFT。** 
>
>    该模式在模式四的基础上,在持续预训练阶段就混入了部分 SFT 数据,可以进一步提高模型的性能。
>
> 6. **模式六:基于对话模型+领域任务 SFT。** 
>
>    该模式直接在预训练和 SFT 得到的对话模型上,使用领域任务数据进行 SFT。这种模式可以继承对话模型已有的对话能力和通用能力。
>
> 7. **模式七:基于对话模型+领域数据持续预训练+领域任务 SFT。** 
>
>    该模式在模式六的基础上,先在对话模型上使用领域数据进行持续预训练,然后再进行领域任务的 SFT。
>
> 在选择训练模式时,我们需要考虑以下几个因素:
>
> 1. **是否需要持续预训练?** 
>
>    大型语言模型的知识主要来自于预训练阶段。如果领域任务数据与预训练数据**差异较大**(例如领域数据来自公司内部,预训练样本无法覆盖),那么进行持续预训练就非常有必要。此外,如果**领域任务数据量较大**(token 数在 1B 以上),并且我们只追求领域任务的效果而不考虑通用能力,那么也建议进行持续预训练。
>
> 2. **是选择对话模型还是基础模型?** 
>
>    如果我们有一个性能优秀的基础模型,那么在基础模型上进行领域数据的 SFT 与在对话模型上进行 SFT,效果差异通常不大。
>
>    但需要注意的是,基于对话模型进行领域 SFT 可能会导致**灾难性遗忘[Catastrophic Forgetting]**,即模型的通用能力会有所下降。如果我们只追求领域任务的效果,那么这一点可以不用考虑;但如果我们既希望提高领域任务效果,又希望保持模型的通用能力,那么建议选择基础模型作为基座模型,在其上进行多任务混合训练。在进行混合训练时,我们需要仔细调节各任务间的数据配比,以达到最佳的平衡。
>
> 3. **其他考虑因素。** 
>
>    1. 在算力资源充足的情况下,如果我们只考虑领域任务效果,可以选择模式二;
>    2. 如果我们希望兼顾模型的综合能力,可以选择模式五。
>    3. 在算力资源受限的情况下,模式六是一个不错的选择。
>
>
> 总之,选择合适的训练模式需要综合考虑任务需求、数据特点和算力资源等因素。通过合理地设计训练流程和数据,我们可以充分发挥大型语言模型的潜力,在特定领域任务上取得优异的性能。

### 2 Self Instruction

在大型语言模型(LLM)的训练过程中,自指导学习[Self-Instruction Learning]是一个关键的步骤, 尤其是在指令微调阶段[Instruction Tuning Stage]。为了让模型能够理解并响应人类的自然语言指令,我们需要准备大量的指令-响应语料。这些语料需要覆盖各种可能的用户询问,以及相应的高质量答案。

> 在 [[InstructGPT Paper](arxiv.org/pdf/2203.02155.pdf)] 中，使用了 1.3w 的数据来对 GPT-3.5 进行监督学习（下图中左 SFT Data）：
>
> <img src="assets\image-20240318144649643.png" alt="image-20240318144649643" style="zoom:33%;" /> 
>
> 可以观察到，数据集中人工标注（labeler）占大头，这还仅仅只是 InstructGPT，和 ChatGPT 远远不是一个量级。

手工标注如此庞大的语料不仅耗时耗力,还需要训练有素的标注团队,以保证数据的一致性和质量。而利用现有的强大语言模型如ChatGPT进行数据蒸馏[Data Distillation],则是一个更加高效且可行的思路。这一思路在学术界已有广泛探索,并取得了不错的效果,如 [Stanford Alpaca](github.com/tatsu-lab/stanford_alpaca) 和 [BELLE](github.com/LianjiaTech/BELLE) 等项目。

这些项目的核心思想是:先准备少量的种子指令作为示例,然后利用ChatGPT的续写能力,生成海量的指令-响应对。种子指令涵盖了不同的任务领域、难度梯度和语言风格,引导ChatGPT探索更广阔的指令空间。ChatGPT生成的语料不仅在数量上远超人工标注,在质量上也能达到相当水准,非常适合作为后续模型训练的数据集。

当然,ChatGPT生成的数据并非完美,其中难免存在错误、偏差或不一致的情况。因此,在实践中还需要对生成的语料进行必要的清洗和过滤。此外,为进一步提升模型在长难句、逻辑推理等方面的能力,还可以专门设计一些高难度的种子指令。

总之,大型语言模型的自指导学习是一个迭代优化的过程。通过借助强大的基础模型,我们可以用较小的代价获得高质量的训练数据,进而训练出更加智能、鲁棒的对话系统,造福实际应用。

```
你被要求提供10个多样化的任务指令。这些任务指令将被提供给GPT模型，我们将评估GPT模型完成指令的能力。
以下是你提供指令需要满足的要求：

	1.尽量不要在每个指令中重复动词，要最大化指令的多样性。
	2.使用指令的语气也应该多样化。例如，将问题与祈使句结合起来。
	3.指令类型应该是多样化的，包括各种类型的任务，类别种类例如：brainstorming，open QA，closed QA，rewrite，extract，generation，classification，chat，summarization。
	4.GPT语言模型应该能够完成这些指令。例如，不要要求助手创建任何视觉或音频输出。例如，不要要求助手在下午5点叫醒你或设置提醒，因为它无法执行任何操作。例如，指令不应该和音频、视频、图片、链接相关，因为GPT模型无法执行这个操作。
	5.指令用中文书写，指令应该是1到2个句子，允许使用祈使句或问句。
	6.你应该给指令生成适当的输入，输入字段应包含为指令提供的具体示例，它应该涉及现实数据，不应包含简单的占位符。输入应提供充实的内容，使指令具有挑战性。
	7.并非所有指令都需要输入。例如，当指令询问一些常识信息，比如“世界上最高的山峰是什么”，不需要提供具体的上下文。在这种情况下，我们只需在输入字段中放置“<无输入>”。当输入需要提供一些文本素材（例如文章，文章链接）时，就在输入部分直接提供一些样例。当输入需要提供音频、图片、视频或者链接时，则不是满足要求的指令。
	8.输出应该是针对指令和输入的恰当回答。 

下面是10个任务指令的列表：
###
1. 指令: 在面试中如何回答这个问题？
1. 输入:当你在车里独处时，你会想些什么？
1. 输出:如果是在晚上，我通常会考虑我今天所取得的进步，如果是在早上，我会思考如何做到最好。我也会尝试练习感恩和活在当下的状态，以避免分心驾驶。
###
2. 指令: 按人口对这些国家进行排名。
2. 输入:巴西，中国，美国，日本，加拿大，澳大利亚
2. 输出:中国，美国，巴西，日本，加拿大，澳大利亚
###
3. 指令:
```

### 3 SFT 数据

#### 3.1 SFT 垂直领域 Pipline

在构建特定领域的大语言模型时**,全面收集并分析用户的问题表达方式**至关重要。**不同领域的用户在描述问题时,往往会使用独特的术语、句式和思维方式。倘若我们只依赖通用领域的问题模式,很可能无法充分理解和满足这些用户的需求**。

为了获取高质量的领域语料,我们可以采用如下的数据收集和模型优化流程:

1. 首先,通过访谈、调研等方式,**收集特定领域用户的真实问题和期望答案**, 深入了解该领域的语言特点, 获得具有代表性的种子指令;
2. 其次, **利用ChatGPT等强大的语言模型, 以种子指令为基础, 生成大规模的指令-响应对**, 显著扩充训练数据的规模和多样性, 同时注意过滤低质数据; 
3. 再次,使用收集和生成的数据对模型进行指令微调(Instruction Fine-tuning, SFT)训练,使其初步习得领域知识和问答模式;
4. 最后,将训练后的模型部署到实际应用中, 收集用户的真实反馈和询问数据,全面评估模型的性能,发现其在实际使用中的优势和不足。

上述流程并非一蹴而就,而是一个持续优化的闭环。我们需要基于用户反馈不断迭代数据和模型,同时兼顾数据的隐私安全、模型的推理效率等问题。专家知识的引入也是不可或缺的,特别是在医疗、法律等高风险领域。

总之,构建领域大语言模型的关键在于全面了解用户需求,灵活运用人机协作,持续优化迭代,最终实现"用户问题-模型回答"的无缝对齐。这是实现高质量人机交互的重要途径,对于推动人工智能技术在垂直领域的应用具有重要意义。

#### 3.2 SFT 训练规模

对于不同规模的语言模型,其所需的指令微调数据量会有所差异。一般而言**,模型规模越大,其少样本学习[Few-Shot Learning]的能力就越强,所需的训练数据量就越少**。但这并非绝对,还需要考虑模型的**预训练质量**、**指令数据的覆盖范围**等因素。

根据现有的研究,对于百亿到千亿参数量级的模型,大概需要**几十万到上百万的指令-响应**对进行微调。而对于万亿参数以上的超大模型,合适的数据量还有待进一步探索。*需要注意的是,**单纯追求数据量而忽视数据质量,很容易导致模型过拟合**或学习到错误的模式。因此,在实践中还需要权衡数据规模与质量的平衡。* | [ 也不一定: Yi-34B-chat的技术报告上周公开了，里面参考LIMA精挑细选了 10K 个sft微调数据， 像34B 1W 的sft数据 ， 有epoch的超参好推荐嘛； 另一方面， （可是LIMA里面说1K就很好了（极端精挑细选的数据） 里面说2k 结果差不多 ； 那对于普通人来说 2~3k 至少也不会比1K差 ， 这也能解释Yi-34B 用了10K ]

> #### 人工标注的意义
>
> 首先,人工标注可以提供更加准确、细粒度的语义理解和知识表示。机器生成的指令-响应对虽然在流畅度上不逊于人类,但在**逻辑一致性、事实准确性**等方面还有待提高。引入人工标注数据,可以帮助模型学习到更加高层的语义信息和推理能力。
>
> 其次,针对**特定领域**(如医疗、法律)或特殊任务(如事实导向型问答),人工标注可以提供**更专业、更符合要求的训练数据**。这些**领域知识或任务细节往往难以简单地通过种子指令涵盖**,需要领域专家的参与标注。
>
> 此外,人工标注还可以作为机器标注的补充和校验。通过对比人机标注结果的差异,我们可以**发现机器生成数据的局限性**,并不断迭代优化数据构建的策略。
>
> 综上,虽然人工标注的成本较高,但其必要性和重要性仍不容忽视。在构建实际应用时,建议采用人机结合的方式,发挥两者的互补优势,以获得更加优质、全面的训练数据

#### 3.3 开源数据集整理

> [大语言模型指令调优综述  包括众多开源指令数据集 ](https://hub.baai.ac.cn/view/31193) 

在这一章中，我们将梳理一些开源的 Instruction Tuning 的数据集，除了直接拿来用以外，我们期望通过分析这些已有数据集，从而学习如何构建一个指令数据集.

| 数据集 | 数据分布                                                     |
| ------ | ------------------------------------------------------------ |
| Alpaca | <img src="assets\v2-90866e67c5ae6f0e383db95db5bcc0a4_720w.webp" alt="img" style="zoom: 33%;" /> |
| BELLE  | <img src="assets\v2-6698ae5b6e3d6b7485ea66f82726f428_720w.webp" alt="img" style="zoom:33%;" /> |

##### ① Alpaca

[[stanford_alpaca](https://link.zhihu.com/?target=https%3A//github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)] 采用上述的 **self instruction** 的方式采集了 5200 条指令训练数据集。

数据样例如下：

```python
{
    "instruction": "Arrange the words in the given sentence to form a grammatically correct sentence.",
    "input": "quickly the brown fox jumped",
    "output": "The quick brown fox jumped quickly."
}
```

其中，instruction 代表要求模型做的任务，input 代表用户输入， output 代表喂给模型的 label。

Alpaca 覆盖了多种类型的指令，包含写作、描述、分类、摘要等多种类型的任务集合。

##### ② BELLE

BELLE 开放了好几种规模的数据集，[[100万](huggingface.co/datasets/BelleGroup/train_1M_CN)]、[[200万](huggingface.co/datasets/BelleGroup/train_2M_CN)]、[[350万](huggingface.co/datasets/BelleGroup/train_3.5M_CN)] 等。

训练数据集样例如下：

```python
{
    "instruction": "判断给定的文章是否符合语法规则。如果不符合，请提供修改建议。 下面是一篇文章的开头: ‘为了探讨这个主题，本文将提供一系列数据和实例，以证明这一观点。’",
    "input": "",
    "output": "这个开头符合语法规则。"
}
```

由于数据量很大，我们无法统计出训练数据集中各任务的真实占比，

但从 [[1000条评测集](github.com/LianjiaTech/BELLE/blob/main/eval/eval_set.json)] 数据分布可以推测出，训练数据集中同样包含：摘要、问答、分类等任务。

我们按照类别对评测数据进行采样，结果如下：

| 任务名称    | 例子                                                         |
| ----------- | ------------------------------------------------------------ |
| 文本生成    | 为一种智能手表编写用户手册，包括详细的使用说明和操作步骤。   |
| 头脑风暴    | 针对给定的主题，进行头脑风暴并记录所有想法。 如何提高公司的销售额？ |
| 开放域问答  | 用一两句话描述著名的尼罗河是如何形成的。                     |
| 封闭域问答  | 从以下选项中选择正确的词汇填空以完整下面的句子。 他喜欢去_______看电影。A) 邮局 B）超市 C）电影院 D）音乐会 |
| 分类        | 请将以下这篇文章分类为新闻报道、科学文章或社论。 据媒体新闻援引美国福克斯新闻网报道，美国伯克希尔哈撒韦公司首席执行官、著名投资人巴菲特近日就美国银行业危机与总统拜登的团队进行对话。 |
| 抽取        | 基于以下表格，请问张三的考勤情况 员工姓名,日期,上班时间,下班时间,是否迟到,是否早退,是否请假 张三,1月1日,8:30,17:30,否,否,否 李四,1月1日,9:00,18:00,是,否,否 王五,1月1日,8:00,16:30,否,是,否 赵六,1月1日,8:30,17:00,否,否,是 张三,1月2日,8:00,17:00,否,否,否 李四,1月2日,8:30,17:30,否,否,否 王五,1月2日,9:00,18:00,是,否,否 赵六,1月2日,8:30,17:00,否,否,是 |
| 重写        | 根据提供的文本重写其中的一段，使之更加简明扼要，同时不丢失原文本的主要信息。 纽约市，简称“纽约”，通常被称为“大苹果”，是美国最大的城市，也是全世界最大的城市之一。位于美国东海岸，东北部边界是大西洋，在新泽西州的东南部。 |
| 摘要        | 基于下面的这个故事，总结其中最重要的三个事件。 小明是一个好学生，每天早上都要起得很早去上学。有一天，他迟到了，因为他的家里来了一个客人。晚上，他参加了一次班级会议，会议主题是如何提高学习效率。回到家后，他又花了一些时间复习功课。 |
| Code & Math | 按照以下要求，写一个SQL查询语句：从表中查找所有性别为女性的学生的姓名和学号。 SELECT name, id FROM students WHERE gender = '女性' |

### 4 SFT 训练

#### 4.1 数据组织

在进行指令微调[Instruction Tuning]的过程中,我们需要对收集到的指令-响应数据进行适当的处理和转换,以便于模型的学习和优化。

具体而言,主要涉及以下几个步骤:

首先,将每个指令(Instruction)及其对应的答案(Answer)拼接成一个完整的文本序列。在拼接时,我们通常会加入一些特殊的标记,如"【USER】"表示用户角色,"【BOT】"表示机器人角色,以及"<bos_token>"和"<eos_token>"分别表示序列的开始和结束。这种角色标记和边界标记可以帮助模型更好地理解对话的结构和语义。

以一个翻译任务为例,拼接后的文本格式如下:

```text
<bos_token>【USER】：将下列内容翻译成英语：｛待翻译文本｝<special_token>【BOT】：{翻译结果}<eos_token>
```

其次,我们将拼接后的文本视为一个自回归[Auto-Regressive]的语言建模任务,并采用预训练[Pre-training]的方式进行训练。具体而言,模型的目标是根据已知的上文(即指令部分)预测下一个词(即响应部分)。这与传统的语言模型预训练类似,但在计算损失函数时有所不同。

在指令微调中,我们通常采用交叉熵[Cross-Entropy]作为损失函数。但与预训练不同的是,我们只计算答案部分(即"【BOT】："之后)的损失,而将指令部分的损失屏蔽掉(通过设置ignore_index)。这样做的目的是让模型专注于学习如何根据指令生成恰当的响应,而不是简单地记忆指令-响应对。

通过这种方式,我们可以在海量的指令-响应数据上对模型进行微调,使其学会理解各种自然语言指令,并给出符合要求的响应。

#### 4.2 训练策略

**SFT 新思路 - SFT + LoRA**

全参数SFT与LoRA相结合的微调策略是一种创新的方法,旨在兼顾微调的效果和资源效率。

该方法的核心思想是在训练的不同阶段采用不同的微调方式:

* 在初始阶段(例如前10%-30%的训练步数)采用全参数SFT,让模型快速适应新的任务和数据分布;
* 在后续阶段切换为LoRA,通过添加低秩分解矩阵,减少需要更新的参数量,从而降低资源消耗。

这种分阶段的微调策略具有多个优点。

* 首先,在初始阶段采用全参数SFT可以充分利用模型参数更新幅度大、loss下降快的特点,加速模型的收敛。
* 其次,在后期阶段采用LoRA可以显著减少训练过程中的资源消耗,如加快训练速度,降低对硬件资源的要求。
* 此外,全参数SFT和LoRA的结合还可以在一定程度上缓解过拟合问题,提高模型的泛化能力。

然而,这一方法也存在一些limitations。

* 首先,全参数SFT和LoRA的最佳组合比例可能因任务而异,需要针对具体任务进行调参和实验,以找到最优的切换时机,这增加了一定的实验负担。
* 其次,在某些极端情况下(如训练数据与预训练数据差异很大),LoRA的性能可能无法与全参数微调相比,这时我们可能需要延长全参数SFT的训练时间,或者放弃LoRA。
* 最后,该方法的有效性还有待更大规模的实验验证,以确保其通用性。

尽管存在这些挑战,全参数SFT与LoRA的结合仍然是一个非常有前景的微调策略。它在效果和效率之间取得了良好的平衡,**特别适用于资源受限的场景**。未来的研究方向包括:

* 探索全参数SFT和LoRA之间的最佳耦合方式(如参数共享、梯度传播等);
* 在训练的不同阶段使用不同的学习率策略、优化器、正则化方法等;
* 将该方法与其他微调技术(如Prefix Tuning、P-Tuning等)进行比较和结合。

#### 4.3 超参数策略

在大语言模型的指令微调阶段[Instruction Tuning Stage],超参数的选择对于模型的性能和收敛速度有着重要影响。该段落中提及的学习率[Learning Rate]、预热比例[Warmup Ratio]和训练轮数[Epoch]等超参数设置策略,基本符合当前学术界的共识和实践经验。然而,这些策略仍然较为经验化,缺乏系统性的方法论。以下我将从理论和实践的角度,对这些超参数的设置进行深入分析和补充。

##### ① Learning Rate

学习率是优化过程中最关键的超参数之一。在指令微调阶段,由于训练数据量相对较小,采用较小的学习率有助于避免模型过拟合和不稳定。一般建议将学习率设置为预训练阶段[Pre-training Stage]的0.1倍左右 (SFT数据集不是特别大的情况下), 如在pre-train阶段的学习率为9e-5，则SFT学习率设置为9e-6。

这一策略在实践中被广泛验证,能够有效提高模型的收敛速度和性能。然而,学习率的最优值还取决于其他因素,如优化器类型、Batch大小、模型架构等。因此,在实际应用中,我们还需要通过实验和对比来确定最佳的学习率范围。

##### ② Warmup ratio

**预热比例决定了学习率在训练初期的增长速度**。在指令微调阶段,由于训练步数相对较少,采用较小的预热比例可以让模型更平滑地适应新的任务和数据分布。(通常pre-train训练的warmup_ratio 0.01～0.015之间，warmup-steps在2000左右。在SFT的时候，建议使用更小的ratio，因为相较于pre-train，SFT样本非常小，较小warmup_ratio可以使模型收敛更平滑 )

这一策略与学习率的设置呈正相关: 当学习率较高时, 增大预热比例有助于避免模型在初期震荡。但需要注意的是, 过小的预热比例可能导致模型在前几个Step的损失[Loss]下降缓慢,影响收敛速度。因此,预热比例的选择需要权衡训练效率和模型稳定性。但如果你的学习率设置较大，那可以增大你的warmup_ratio，两者呈正相关。

##### ③ Epoch

训练轮数的设置主要取决于指令微调数据集的大小和模型收敛情况。

对于较小的数据集(如几千到几万条),我们通常需要**更多的训练轮数**,以确保模型充分学习和内化指令。过多的训练轮数也可能导致过拟合,使模型在训练集上表现出色, 但在测试集上泛化能力下降, 但过拟合要优于欠拟合。因此,在实践中,**Epoch设置可以根据loss收敛情况设置**, 

如果SFT样本数量较多，如在十万以上，一般2个epoch即可收敛。

### 5 模型评测

> [PandaLM: 评估大模型的大模型, 保护隐私、可靠、可复现，三行代码即可调用 (zhihu.com)](https://www.zhihu.com/tardis/zm/art/626391857?source_id=1005)
>
> huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard -> LLM Leaderboard

在大语言模型[Large Language Model, LLM]的研发过程中,指令微调阶段[Instruction Tuning Stage]的模型评测一直是一个具有挑战性的问题。与预训练阶段[Pretraining Stage]相比,后者有着相对明确的评价指标,如困惑度[Perplexity, PPL]、负对数似然[Negative Log-Likelihood, NLL]等,而指令微调阶段则缺乏公认的客观评价指标。传统的自然语言生成[Natural Language Generation, NLG]评价指标,如BLEU、ROUGH等,已经不再适用于评估当前快速发展的语言生成模型。

为了解决这一问题,一种比较流行的方式是利用更高级的语言模型,如GPT-4,来对候选模型的生成结果进行评分。这种方法的基本假设是,更高级的语言模型具有更强的判断能力,其判断打分能力要高于其生成能力。类似地,经过训练的普通人能够对文章的优劣进行评价,但他们不一定能够写出相应质量的文章。例如,FastChat项目中就采用了这种方法,利用GPT-4为OpenLlama、ChatGLM、BELLE等开源模型的生成结果打分。

具体的评测流程如下:

1. 对于每一个问题,先获得ChatGPT的回复,以及其他候选模型的回复;
2. 将"ChatGPT答案 - 候选模型答案"这样的配对喂给GPT-4打分(满分为10分);
3. 对每个任务单独进行统计,并在最后一列求得平均值。GPT-4会对每一条测试样本的2个答案分别进行打分,并给出打分理由。

然而,实践中发现,GPT-4给出的分数和理由并不一定正确。例如,GPT-4可能为某个模型的答案打出了更高的分数,给出的理由是将"最长时期"改为"最长时期之一"会更准确。但事实上,如果指令中明确设定就是"最长时期",那么这种"给高分"的理由其实是不正确的。此外,仅仅调换句子顺序也会对最后打分结果产生影响。为了缓解这个问题,可以考虑"调换句子顺序并求和平均"的方法。

尽管如此,GPT-4给出的分数可能并没有我们想象中那么可靠。为此,我们通过人工审核的方式对每个答案进行了复核。在人工评估中,我们主要考虑以下几个维度(按重要性排序):准确性、指令服从性、信息覆盖率、可读性和无害性。

总的来说,指令微调阶段的模型评测仍然是一个开放的研究问题。利用更高级的语言模型进行评分是一种可行的方法,但仍然存在一些局限性。未来,我们需要在这一方法的基础上,开发出更加可靠、客观的评测指标和方法,以推动大语言模型的进一步发展。

> | 步骤                  | 图示                                                         |
> | --------------------- | ------------------------------------------------------------ |
> | STEP 2: GPT-4 打分    | <img src="assets\v2-30369e5361b3097e6c9c3254c1f9b5e4_720w.webp" alt="img" style="zoom: 33%;" /> |
> | STEP 3: 求平均值      | <img src="assets\v2-8adadb3bab29880e9fecacb673da87d2_720w.webp" alt="img" style="zoom: 33%;" /> |
> | STEP 4: 人工的 Review | <img src="assets\v2-73395d9c1c2bb8b8376009716428f530_720w.webp" alt="img" style="zoom: 33%;" /> |

> #### GPT打分的局限性与优化策略
>
> 在指令微调阶段,利用更高级的语言模型如GPT对候选模型的生成结果进行评分,是一种常用的评测方法。这种方法基于这样一个假设:更高级的语言模型具有更强的判断能力,其判断打分能力要高于其生成能力。然而,这种方法也存在一些局限性:
>
> 1. 首先,GPT缺乏领域知识,可能会根据自身知识库进行判断。如果直接使用通用的GPT模型进行打分,它可能会倾向于选择与自身知识库相符的答案,而非根据任务目标进行客观评判。为了克服这一问题,我们可以考虑在评测之前,先对GPT进行领域微调[Domain Adaptation],使其掌握该领域的基本知识,从而能够更加客观地评判答案的质量。
>
> 2. 其次,在指令微调阶段,我们应该更加关注模型的指令遵循能力,而非事实准确性。指令微调阶段的主要目标是提高模型对指令的理解和执行能力,因此在评测时,我们应该更加关注模型是否准确理解并执行了指令,而非过于苛求生成内容的事实准确性。当然,这并不意味着我们可以完全忽略事实准确性,毕竟生成的内容如果与事实相悖,也会影响模型的可用性。
>
> 3. 此外,表述语气和事实性之间的关系也值得我们思考。一般来说,表述语气与事实性并没有必然的联系。但在某些情况下,表述语气可能会影响我们对事实性的判断。因此,在评测时,我们应该尽量分开考虑表述语气和事实性,独立给出评分,以免相互影响。
>
> 为了获得更加客观、准确的评测结果,我们可以考虑先对GPT进行领域微调,并在评测时区分不同的评判维度,如指令遵循能力、表述语气、事实准确性等,分别进行评分。同时,我们也应该意识到,尽管GPT打分存在一定的局限性,但它仍然是一种有效的评测方法,可以为我们提供有价值的参考

### 6 经验 OR 信息

* ① 如果SFT任务类型较多，可以尝试添加system_prompt，不同的任务使用不同的system_prompt；

* ② 一个好的基座模型非常重要

* ③ 在SFT的时候，loss依然是你最重要的指标！一般在SFT过程中，loss会先升后降；

* ④ 可以尝试多种模式训练方案，如在continue pre-train 中添加SFT数据，在SFT数据添加高质量的pre-train数据；

* ⑤ 模型的参数量非常重要；

* ⑥ 在目前开源的大模型里面有三个基座模型表现非常好 | 

  * [通义千问1.5-72B-Chat](https://link.zhihu.com/?target=https%3A//modelscope.cn/models/qwen/Qwen1.5-72B-Chat/summary)
  * [Yi-34B-Chat](https://link.zhihu.com/?target=https%3A//modelscope.cn/models/01ai/Yi-34B-Chat/summary)
  * [mistralai/Mixtral-8x7B-Instruct-v0.1 · Hugging Face](https://link.zhihu.com/?target=https%3A//huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)

  目前发现通义千问1.5-72B-Chat效果非常好，通过continue-pre-train之后的[Mixtral-8x7B](https://link.zhihu.com/?target=https%3A//huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) 效果也非常好。

## STEP 3 强化学习人类反馈阶段 

在完成监督微调[Supervised Fine-Tuning, SFT]后,我们通常可以得到一个性能不错的模型。但是,在 SFT 阶段,我们主要是通过标注数据告诉模型什么是「好」的,却没有明确指出什么是「不好」的。**由于 SFT 数据有限** ,我们对模型的引导能力也受到限制。这可能导致预训练模型中原有的「错误」或「有害」知识没能在 SFT 阶段得到纠正, 从而出现「有害性」或「幻觉」等问题。也就是**模型并不清楚什么答案是好的答案, 什么答案是不好的答案, 什么答案是更好的答案.** 

为了解决这个问题, **人们训练出奖励模型**. 奖励模型通过学习人类偏好，可以对语言模型生成的文本进行打分，为 RLHF 和 DPO 提供评估依据。RLHF 利用奖励模型的打分结果，通过强化学习的方式优化语言模型的策略，使其更倾向于生成高分文本。同时强化学习是一种只告诉模型「好不好」，而不直接告诉模型「怎样才能变好」的学习方式。**这种方式给予模型更大的探索自由度，有助于突破监督学习的性能上限。**

### 1 奖励模型（Reward Model）

> [【RLHF】想训练ChatGPT？得先弄明白Reward Model怎么训（附源码） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/595579042)

#### 1.1 利用偏序对训练奖励模型

在近年来的大语言模型[Large Language Model, LLM]研究中,利用**偏序对[Pairwise Preference]**来训练奖励模型[Reward Model]已经成为一种常见且有效的方法。这一方法最早由OpenAI在其总结模型 [[Summarization](arxiv.org/pdf/2009.01325.pdf)]和指令GPT [[InstructGPT](arxiv.org/pdf/2203.02155.pdf)] 的论文中提出并应用。

传统的监督学习方法通常需要为每一个训练样本标注一个确定的分数或类别,而偏序对的思路则不同。它通过标注一组样本的**相对好坏顺序**,让模型学习如何判断句子质量的高低。具体来说,给定一组句子,标注人员会按照一定的标准,如流畅度、连贯性、信息量等,将它们按照从好到坏的顺序排列。这样,模型就可以通过学习最大化好句子和坏句子之间的得分差异,来掌握如何自动评估一个句子的质量。

相比直接打分,偏序对学习有几个明显的优势:

1. 标注成本更低:

   对于人类标注者来说,判断句子的相对好坏要比给出一个精确的分数更加容易和直观。这样可以显著降低数据标注的成本。

2. 模型泛化能力更强:

   通过学习句子的相对优劣关系,模型可以更好地掌握判断句子质量的一般规律,从而在面对新的未见过的句子时,也能做出合理的判断。

3. 更符合人类偏好:

   偏序对的学习目标就是让模型的判断与人类的偏好尽可能一致。这有助于模型生成更加符合人类期望的文本。

在实践中,偏序对训练通常采用排序学习[Learning to Rank]的思路,通过构建合适的损失函数,如排序网络[RankNet]或 RankSVM,来最小化模型预测的偏序关系与真实标注之间的差异。一旦奖励模型学会了对句子质量进行打分,它就可以被用于指导LLM的生成过程,使其倾向于生成高质量、符合人类偏好的文本。这一过程通常通过强化学习[Reinforcement Learning]来实现,即将奖励模型的打分作为强化学习的奖励信号,引导LLM优化其生成策略。
$$
\operatorname{loss}(\theta)=-\frac{1}{\left(\begin{array}{c}
K \\
2
\end{array}\right)} E_{\left(x, y_w, y_l\right) \sim D}\left[\log \left(\sigma\left(r_\theta\left(x, y_w\right)-r_\theta\left(x, y_l\right)\right)\right)\right]
$$

> 这个公式表示了偏序对学习中常用的交叉熵损失函数。下面我将逐步解释每个部分的含义:
>
> 1. $\theta$: 表示奖励模型的参数。
> 2. $K$: 表示每个训练样本中包含的候选响应的数量。
> 3. $\left(\begin{array}{c} K \\ 2 \end{array}\right)$: 表示从 $K$ 个候选响应中选择 2 个的组合数,即 $\frac{K(K-1)}{2}$。
> 4. $D$: 表示训练数据集,其中每个样本包含一个上下文 $x$ 以及 $K$ 个候选响应 $\{y_1, y_2, ..., y_K\}$。
> 5. $\left(x, y_w, y_l\right) \sim D$: 表示从数据集 $D$ 中采样一个样本,其中 $x$ 为上下文,$y_w$ 为优质响应(winning response),$y_l$ 为劣质响应(losing response)。
> 6. $r_\theta\left(x, y\right)$: 表示奖励模型对于给定上下文 $x$ 和响应 $y$ 的打分。
> 7. $\sigma(\cdot)$: 表示 Sigmoid 函数,用于将分数差转化为概率。
> 8. $\log \left(\sigma\left(r_\theta\left(x, y_w\right)-r_\theta\left(x, y_l\right)\right)\right)$: 表示优质响应得分高于劣质响应得分的对数概率。
>
> 整个公式的含义如下:对于数据集中的每个样本,我们随机选择两个候选响应 $y_w$ 和 $y_l$,然后计算奖励模型对它们的打分差异。通过最大化优质响应得分高于劣质响应得分的对数概率,模型学习对候选响应进行排序。由于每个样本有 $\left(\begin{array}{c} K \\ 2 \end{array}\right)$ 种可能的响应对,因此损失函数对所有可能的响应对取平均。
>
> 最小化该损失函数意味着最大化奖励模型将优质响应排在劣质响应之前的概率。通过这种方式,奖励模型能够学习到人类偏好的隐式表示,进而对任意输入的响应进行打分。
>
> 
>
> 让我们来看一个具体的训练样本实例。假设我们有以下数据:
>
> 上下文 $x$:
> ```
> 用户:我最近感到非常疲惫和没有动力,你有什么建议吗?
> ```
>
> 候选响应 $\{y_1, y_2, y_3, y_4\}$:
> ```
> y_1: 你可能需要休息一下,为什么不尝试放松片刻,做一些你喜欢的事情呢?保重身体健康也很重要,确保你获得了足够的睡眠和营养。如果这种状况持续很长时间,建议你咨询医生以排除潜在的健康问题。
> y_2: 振作起来,每个人都会有低谷的时候。
> y_3: 多锻炼,多吃蔬菜水果,早睡早起,你会感到精力充沛的。
> y_4: 我完全理解你的感受,生活和工作的压力常常让人感到疲惫和倦怠。以下是一些建议:1)确保充足的睡眠;2)均衡饮食,避免高糖高脂食物;3)适度运动,如散步、瑜伽等;4)给自己一些放松的时间,做喜欢的事情;5)与友人交流,必要时寻求帮助。记住,照顾好自己是最重要的。
> ```
>
> 假设经过人工标注,我们得到以下偏序对:
> ```
> y_4 > y_1 > y_3 > y_2
> ```
>
> 这意味着标注人员认为 $y_4$ 是最佳响应,$y_1$ 次之,$y_3$ 和 $y_2$ 依次排在后面。
>
> 在训练过程中,我们随机选择两个候选响应形成一个偏序对,例如 $(y_4, y_2)$。然后,我们计算奖励模型对这两个响应的打分差异:
> $$
> r_\theta(x, y_4) - r_\theta(x, y_2)
> $$
>
> 接下来,我们将打分差异输入 Sigmoid 函数,得到 $y_4$ 优于 $y_2$ 的概率:
> $$
> \sigma(r_\theta(x, y_4) - r_\theta(x, y_2))
> $$
>
> 最后,我们计算该概率的对数,作为这个偏序对的损失:
> $$
> \log(\sigma(r_\theta(x, y_4) - r_\theta(x, y_2)))
> $$
>
> 我们对所有可能的偏序对 $(y_4, y_2), (y_4, y_3), (y_4, y_1), (y_1, y_2), (y_1, y_3), (y_3, y_2)$ 重复上述过程,并取平均值作为最终的损失。通过最小化这个损失函数,奖励模型学习对候选响应进行排序,使得排序结果尽可能符合人类偏好。
>
> 这就是一个偏序对学习的训练样本实例。在实际应用中,我们通常需要大量的标注数据来训练一个鲁棒的奖励模型。

#### 1.2 训练数据

在 OpenAI 的 Summarize 任务中，研究人员使用了 6.4 万条[[人工标注的偏序对 (human-labeled preference pairs)](github.com/openai/summarize-from-feedback)]进行训练。而在 InstructGPT 任务中，则采用了 3.2 万条人工标注的偏序对 [4~9] 来优化模型。这些偏序对通过让标注者在模型生成的两个不同输出之间进行选择而获得。

<img src="assets\v2-c1fcdbd0d76cc5ef045b64e28a840ec3_720w.webp" alt="img" style="zoom: 67%;" />

除了 OpenAI 之外，最近 Hugging Face 发布的 [[StackLlama](huggingface.co/blog/zh/stackllama)] 也采用了类似的方法。在这项工作中，研究人员利用从 [[Stack Exchange](stackexchange.com/)] 问答网站上抽取的 10 万条偏序对来训练奖励模型 [Reward Model]，以此来引导语言模型 [Language Model, LM] 生成更加符合人类偏好的回答。 

通过对比以上几个项目，我们可以发现，目前学界对于训练一个稳定的奖励模型所需的最小数据量还没有形成统一的认识，具体的数据需求可能会因任务的不同而有所差异。

不过，综合已有的实践经验来看，**5 万条以上的人工标注偏序对**可能是一个相对保险的数据量级。这个数量能够为奖励模型提供充足的训练信号，使其能够较好地理解和判断语言模型生成内容的质量，从而有效地指导语言模型进行优化。

#### 1.3 模型大小

奖励模型的主要作用是对语言模型生成的内容进行打分，因此其规模只需要足以理解和评估生成内容即可，并不一定要和语言模型一样大。

事实上，在已有的研究中，奖励模型的规模选择上也没有一个统一的标准：

* 在 OpenAI 的 Summarize 任务中，研究人员使用了一个 6B 参数的奖励模型，配合同样是 6B 参数的语言模型。

* 而在 InstructGPT 项目中，奖励模型的规模同样是 6B，但语言模型的参数量却高达 175B。

* DeepMind 在其研究中则选择了保持奖励模型和语言模型参数量一致，均为 70B。

一种直观的理解是，相比于生成任务，对生成内容进行判别和打分可能是一个相对更简单的任务。因此，在计算资源允许的情况下，可以考虑用比语言模型稍小的模型来充当奖励模型，以节省训练成本。

不过这一点也不是绝对的，具体的模型规模还是需要根据任务的难度和复杂程度来进行权衡。一些较难的判别任务可能同样需要大规模的奖励模型才能达到理想的效果。

总的来说，奖励模型的规模选择是一个需要根据具体情况进行平衡的问题。通过对比几个具有代表性的项目，我们可以获得一些有益的启发和参考，但最终还是需要根据手头的任务和资源限制来作出恰当的决策。

### 2 利用奖励模型优化LLM的主流方法: BON、DPO和PPO

在优化大语言模型(LLM)时,奖励模型(Reward Model, RM)起着至关重要的作用。奖励模型能够对模型的输出质量进行评估,为模型的优化提供反馈和指导。

目前,业界主流的三种利用奖励模型优化LLM的方法分别为Best-of-N(BON)、Direct Preference Optimisation(DPO)和Proximal Policy Optimization(PPO)。本文将对这三种方法的原理、数据使用、优缺点等方面进行详细介绍和比较。

#### 2.1 BON (Best-of-N)

BON是一种**基于搜索**的优化方法。

其基本思路是,对于每个输入,模型生成N个候选输出,然后使用奖励模型对这N个输出进行评分,选择得分最高的输出作为最终结果。通过这种方式,模型可以在生成过程中不断尝试和调整,最终找到奖励模型认为最优的输出。

BON的数据使用方式较为简单,仅需要输入数据和对应的候选输出即可。在训练时,模型根据输, 通过设置 **temperature 值**让同一个模型生成生成N个候选,奖励模型对候选打分,并将得分最高的候选作为正例,其余候选作为负例,形成训练数据。这些数据可以用于**微调模型**,使其学会生成奖励模型认为高质量的输出。

> 即 Sample -> SFT -> Sample -> ... 的 Pipline

在 [[Llama2 Paper](arxiv.org/pdf/2307.09288.pdf)] 中使用了这种方法, 论文中指出：在进行 SFT 时，应当使用之前所有策略下的 Good Samples（而非仅是最近一次策略模型 Sample 出的样本），以提高模型的泛化性。

> arxiv.org/pdf/2210.10760.pdf - OpenAI
>
> BON和PPO是两种常用的生成式语言模型优化方法,它们在探索广度、进化深度和训练效果上各有特点。BON在每一轮迭代中进行广泛探索,通过采样多个候选答案来更全面地发掘答案空间,并具有较为稳定的训练过程。而PPO则采用**多轮次**的"采样-进化"迭代,在原始模型的基础上进行更深入的梯度更新和策略优化,虽然训练过程可能较为不稳定,但往往能达到更高的性能上限。

BON的优点是简单易实现,且可解释性强,我们可以直观地看到模型生成的不同候选及其对应的质量评分。但其缺点也显而易见,即**计算开销大**,因为**每个输入都需要生成和评估N个候选**。此外,由于**候选生成是离散的**,BON很难对模型进行细粒度的优化。

#### 2.2 DPO (Direct Preference Optimisation)

DPO是一种**无需显式构建奖励模型**的训练方法,它直接利用人工标注的偏序对(preference pairs)来优化语言模型[1]。DPO借鉴了对比学习(contrastive learning)[2]的思路,其核心思想是(在给定同一输入(prompt)的情况下,最大化模型对正例(selected answer)和负例(rejected answer)的生成概率差距。

(也就是说, 我跳过Reward Model 这一步, 直接找出人的偏好, 即确定模型应该输出什么样的答案 不应该输出什么样的答案,. )

##### 2.2.1 DPO的优化目标

DPO的优化目标可以用以下公式表示 [[这篇 paper](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.12036.pdf)] : 

$$
\begin{array}{r}
\min _\pi \underset{\substack{x \sim \rho \\
y, y^{\prime} \sim \mu}}{\mathbb{E}}\left[-p^*\left(y \succ y^{\prime} \mid x\right) \log \sigma\left(\tau \log \left(\frac{\pi(y \mid x)}{\pi\left(y^{\prime} \mid x\right)}\right)-\right.\right. 
\left.\left.\tau \log \left(\frac{\pi_{\text {ref }}(y \mid x)}{\pi_{\text {ref }}\left(y^{\prime} \mid x\right)}\right)\right)\right] .
\end{array}
$$

其中,$\pi$表示要优化的语言模型(policy), $\rho$表示输入的分布, $\mu$表示偏序对的分布,$p^*(y \succ y' | x)$表示在给定输入$x$的情况下$y$优于$y'$的概率, $\sigma$是sigmoid函数,$\tau$是温度超参数,$\pi_{\text{ref}}$是一个固定的参考语言模型。

这个目标函数由两部分组成:

1. $\tau \log \left(\frac{\pi(y \mid x)}{\pi\left(y^{\prime} \mid x\right)}\right)$表示优化语言模型$\pi$在正例$y$和负例$y'$上的对数概率比值,这一项鼓励模型提高正例的生成概率,降低负例的生成概率。

2. $-\tau \log \left(\frac{\pi_{\text{ref}}(y \mid x)}{\pi_{\text{ref}}\left(y^{\prime} \mid x\right)}\right)$表示参考语言模型$\pi_{\text{ref}}$在正例$y$和负例$y'$上的对数概率比值,作为一个基准。

目标函数的总体思路是,让优化模型的对数概率比值尽可能接近参考模型的对数概率比值,同时考虑偏序对的权重$p^*(y \succ y' | x)$。温度参数$\tau$控制优化强度,较大的$\tau$会放大正负例之间的差异,较小的$\tau$则会缩小差异[3]。

##### 2.2.2 **DPO的数据集构建和训练细节**

###### ① 构建DPO训练数据集

DPO的训练数据集由一组偏序对$(x, y, y')$组成,其中$x$表示输入(prompt),$y$和$y'$表示两个不同的模型输出,且$y$优于$y'$。为了构建这样的数据集,我们可以采取以下步骤:

1. 收集一组输入样本$\{x_1, x_2, ..., x_n\}$,覆盖目标任务的不同方面和难度水平。这些样本可以来自现有的数据集,或者通过人工设计、爬取等方式获得。

2. 对于每个输入样本$x_i$,使用当前的语言模型(或多个不同的语言模型)生成一组候选输出$\{y_{i1}, y_{i2}, ..., y_{im}\}$

3. 对于每个输入样本$x_i$,让人类标注者比较和选择候选输出,构建偏序对$(x_i, y_{ij}, y_{ik})$,表示在输入$x_i$下,$y_{ij}$优于$y_{ik}$。为了提高数据质量,可以让多个标注者对同一组候选进行评判,并采用投票、加权平均等方式合并结果。

4. 过滤和平衡数据集,确保正负例的比例合适,并且不同难度和类型的输入都有充分的覆盖。

举个例子,假设我们要训练一个问答模型,输入样本$x_i$可以是"What is the capital of France?",候选输出$y_{i1}$、$y_{i2}$、$y_{i3}$分别为:

- $y_{i1}$: "The capital of France is Paris."
- $y_{i2}$: "Paris is the capital of France."
- $y_{i3}$: "The capital of Italy is Rome."

人类标注者可能会认为$y_{i1}$和$y_{i2}$都是正确答案,且优于$y_{i3}$,因此我们可以构建两个偏序对:$(x_i, y_{i1}, y_{i3})$和$(x_i, y_{i2}, y_{i3})$

###### ② DPO的训练过程

有了偏序对数据集后,我们就可以开始训练DPO模型了。与SFT不同,DPO并不直接优化输出的对数似然,而是优化正负例之间的相对偏好。训练的核心是最小化之前给出的目标函数:
$$
\begin{array}{r}
\min _\pi \underset{\substack{x \sim \rho \\
y, y^{\prime} \sim \mu}}{\mathbb{E}}\left[-p^*\left(y \succ y^{\prime} \mid x\right) \log \sigma\left(\tau \log \left(\frac{\pi(y \mid x)}{\pi\left(y^{\prime} \mid x\right)}\right)-\right.\right. 
\left.\left.\tau \log \left(\frac{\pi_{\text {ref }}(y \mid x)}{\pi_{\text {ref }}\left(y^{\prime} \mid x\right)}\right)\right)\right] .
\end{array}
$$
具体的训练步骤如下:

1. 从偏序对数据集$\mu$中采样一批次的数据$\{(x^{(i)}, y^{(i)}, y'^{(i)})\}_{i=1}^B$,其中$B$为批次大小。

2. 对于每个偏序对$(x^{(i)}, y^{(i)}, y'^{(i)})$,计算当前模型$\pi$和参考模型$\pi_{\text{ref}}$在$y^{(i)}$和$y'^{(i)}$上的条件概率,即$\pi(y^{(i)} \mid x^{(i)})$、$\pi(y'^{(i)} \mid x^{(i)})$、$\pi_{\text{ref}}(y^{(i)} \mid x^{(i)})$、$\pi_{\text{ref}}(y'^{(i)} \mid x^{(i)})$。这可以通过将$y^{(i)}$和$y'^{(i)}$输入到相应的语言模型中,并计算序列的对数似然来实现。

3. 计算每个偏序对的损失:

$$
\begin{aligned}
l^{(i)} = -p^*(y^{(i)} \succ y'^{(i)} \mid x^{(i)}) \log \sigma(\tau \log(\frac{\pi(y^{(i)} \mid x^{(i)})}{\pi(y'^{(i)} \mid x^{(i)})}) - \tau \log(\frac{\pi_{\text{ref}}(y^{(i)} \mid x^{(i)})}{\pi_{\text{ref}}(y'^{(i)} \mid x^{(i)})}))
\end{aligned}
$$

其中$p^*(y^{(i)} \succ y'^{(i)} \mid x^{(i)})$表示偏序对的权重,通常设为1。

4. 对批次内的损失取平均,得到总损失:

$$
L = \frac{1}{B}\sum_{i=1}^B l^{(i)}
$$

5. 计算总损失对模型参数的梯度$\nabla_\pi L$,并使用优化算法(如Adam)更新模型参数,以最小化总损失。

6. 重复步骤1-5,直到模型收敛或达到预设的训练轮数。

与SFT相比,DPO的训练过程需要额外计算参考模型的概率,并引入温度参数$\tau$来控制优化强度。但核心思路仍然是通过最小化一个目标函数来更新模型参数,使其生成的结果符合人类偏好。

> #### 如何计算序列的生成概率
>
> 当然,让我详细解释一下如何将偏序对中的序列输入到语言模型中,并计算它们的对数似然。
>
> 假设我们有一个基于Transformer的语言模型,如GPT-2或BERT,其参数为$\theta$。对于一个长度为$T$的输入序列$\mathbf{x} = (x_1, x_2, ..., x_T)$,语言模型的目标是估计该序列的概率分布$p_\theta(\mathbf{x})$。在实践中,我们通常使用自回归因式分解来计算这个概率[1]:
>
> $$
> p_\theta(\mathbf{x}) = \prod_{t=1}^T p_\theta(x_t \mid x_1, x_2, ..., x_{t-1})
> $$
>
> 其中$p_\theta(x_t \mid x_1, x_2, ..., x_{t-1})$表示在给定前$t-1$个标记的情况下,第$t$个标记为$x_t$的条件概率。语言模型通过最小化负对数似然损失函数来学习这些条件概率:
>
> $$
> \mathcal{L}(\theta) = -\frac{1}{T}\sum_{t=1}^T \log p_\theta(x_t \mid x_1, x_2, ..., x_{t-1})
> $$
>
> 现在,让我们考虑如何将偏序对中的序列$y^{(i)}$和$y'^{(i)}$输入到语言模型中,并计算它们的对数似然。以$y^{(i)}$为例,假设其长度为$T$,表示为$\mathbf{y}^{(i)} = (y^{(i)}_1, y^{(i)}_2, ..., y^{(i)}_T)$。
>
> 1. 首先,我们需要将$\mathbf{y}^{(i)}$转换为语言模型可以处理的格式,通常是一个标记ID序列。这可以通过查询预训练的词表(如 WordPiece、BPE)来实现[2]。例如,如果$y^{(i)}_1$是"The",其在词表中的ID为120,则将其转换为120。重复这个过程,直到获得完整的标记ID序列$\mathbf{y}'^{(i)} = (y'^{(i)}_1, y'^{(i)}_2, ..., y'^{(i)}_T)$。
>
> 2. 然后,我们将标记ID序列$\mathbf{y}'^{(i)}$输入到语言模型中,通过前向传播计算每个位置的条件概率分布。以GPT-2为例,这通常涉及词嵌入层、多个Transformer解码器块和最后一个线性层[3]。
>
> 3. 在前向传播的过程中,我们可以同时计算序列$\mathbf{y}^{(i)}$的对数似然。具体来说,在每个位置$t$,我们提取目标标记$y'^{(i)}_t$对应的对数概率,即$\log p_\theta(y'^{(i)}_t \mid y'^{(i)}_1, y'^{(i)}_2, ..., y'^{(i)}_{t-1})$,并将其累加到对数似然中。
>
> 4. 重复步骤2-3,直到处理完整个序列,得到其对数似然:
>
> $$
> \log p_\theta(\mathbf{y}^{(i)}) = \sum_{t=1}^T \log p_\theta(y'^{(i)}_t \mid y'^{(i)}_1, y'^{(i)}_2, ..., y'^{(i)}_{t-1})
> $$
>
> 5. 对偏序对中的另一个序列$y'^{(i)}$重复步骤1-4,得到其对数似然$\log p_\theta(\mathbf{y}'^{(i)})$。
>
> 6. 最后,我们可以使用$\log p_\theta(\mathbf{y}^{(i)})$和$\log p_\theta(\mathbf{y}'^{(i)})$来计算DPO的损失函数,并进行梯度下降优化。
>
> 需要注意的是,在实际实现中,我们通常会并行处理一个批次的序列,以提高计算效率。这可以通过将标记ID序列打包成一个矩阵,并使用掩码来处理不同长度的序列[4]。
>
> 此外,为了提高训练稳定性和泛化性能,我们还可以采用一些技巧,如学习率调度、梯度裁剪、权重衰减等[5]。
>
> 总的来说,将偏序对中的序列输入到语言模型中并计算对数似然是DPO训练的关键步骤。通过合理设计模型结构和优化算法,我们可以有效地学习人类偏好,并生成高质量的序列输出。
>

###### ③ 代码实现

这段代码实现了DPO的损失函数计算,并返回了损失值以及chosen和rejected样本的奖励。

```python
def dpo_loss(
    self,
    policy_chosen_logps: torch.FloatTensor,   # 当前模型在chosen样本上的对数概率
    policy_rejected_logps: torch.FloatTensor,  # 当前模型在rejected样本上的对数概率
    reference_chosen_logps: torch.FloatTensor, # 参考模型在chosen样本上的对数概率
    reference_rejected_logps: torch.FloatTensor, # 参考模型在rejected样本上的对数概率
    reference_free: bool = False,  # 是否使用无参考模型的设置
) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
    # 计算当前模型和参考模型在chosen和rejected样本上的对数概率差
    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps

    if reference_free:
        # 如果使用无参考模型设置,则将参考模型的对数概率差设为0
        ref_logratios = 0

    # 计算最终的logits,即当前模型和参考模型对数概率差之差
    # 这里的思想是,如果参考模型对两个样本的概率差异也很大,则不要让当前模型的概率差异过大,以防训练不稳定
    logits = pi_logratios - ref_logratios

    if self.loss_type == "sigmoid":
        # 使用sigmoid损失函数,最大化chosen和rejected样本的概率差
        losses = -F.logsigmoid(self.beta * logits)
    elif self.loss_type == "hinge":
        # 使用hinge损失函数
        losses = torch.relu(1 - self.beta * logits)
    else:
        raise ValueError(f"Unknown loss type: {self.loss_type}. Should be one of ['sigmoid', 'hinge']")

    # 计算chosen和rejected样本的奖励,即当前模型和参考模型在这些样本上的对数概率差
    # 这里使用detach()函数,将奖励值从计算图中分离出来,以避免在反向传播时影响梯度计算
    chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()
    rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()

    return losses, chosen_rewards, rejected_rewards
```

这段代码的主要思想是:
1. 计算当前模型和参考模型在chosen和rejected样本上的对数概率差,得到pi_logratios和ref_logratios。
2. 如果使用无参考模型设置,则将ref_logratios设为0。
3. 计算最终的logits,即pi_logratios和ref_logratios的差值。这里的目的是防止当前模型的概率差异过大,导致训练不稳定。
4. 根据指定的损失函数类型(sigmoid或hinge),计算最终的损失值losses。
5. 计算chosen和rejected样本的奖励值,即当前模型和参考模型在这些样本上的对数概率差,并使用detach()函数将其从计算图中分离出来。
6. 返回损失值losses,以及chosen和rejected样本的奖励值chosen_rewards和rejected_rewards。

##### 2.2.3 DPO的优势和局限

相比需要显式奖励模型的方法,DPO的主要优势在于:
1. 直接优化目标任务,避免了因奖励模型偏差导致的目标偏移问题[6]。
2. 减少了对大规模高质量标注数据的需求,因为偏序对的标注成本通常低于绝对质量打分[7]。
3. 可以自然地融入人类反馈,实现交互式和渐进式的模型优化[8]。

但DPO也存在一些局限性:
1. 偏序对的质量和分布对优化效果有很大影响,需要仔细设计数据收集和筛选机制[9]。
2. 模型的优化空间受到参考模型的限制,当参考模型与最优解存在较大差距时,DPO可能无法达到最优性能[10]。
3. 对于一些主观性较强或存在多种正确答案的任务,构建高质量的偏序对可能具有挑战性[11]。

#### 2.3 PPO (Proximal Policy Optimization)

<img src="assets\image-20240319133052269.png" alt="image-20240319133052269" style="zoom: 25%;" /> 

##### 2.3.1 PPO 简介

[[PPO](arxiv.org/pdf/1707.06347.pdf)] （Proximal Policy Optimization）[PPO] 是一种强化学习 [Reinforcement Learning] 中的优化算法，属于基于策略梯度 [Policy Gradient] 的方法，其前身是 TRPO（Trust Region Policy Optimization）[TRPO]。PPO 在 Actor-Critic（演员-评论家）[Actor-Critic] 架构的基础上，引入了重要性采样 [Importance Sampling] 技术，以缓解 on-policy 模型一次采样数据只能更新一次模型的问题，提高了数据利用率和模型训练速度。

PPO 算法的核心思想是在策略更新时，限制新旧策略之间的差异，避免策略更新幅度过大导致性能急剧下降。具体来说，PPO 引入了一个surrogate objective function，用于衡量新旧策略之间的差异，并通过限制该函数的变化范围来控制策略更新的幅度。这种方法使得 PPO 在保证策略稳定性的同时，也能够适应较大的策略更新，从而加快了训练速度。

在 PPO 算法中，重要性采样扮演了关键角色。传统的 on-policy 算法（如 REINFORCE）在每次更新策略时都需要重新采样数据，导致数据利用率低下。而 PPO 通过重要性采样，可以重复利用之前采样的数据，并对其进行加权，以适应当前的策略。这种方法大大提高了数据利用率，减少了采样次数，加速了模型训练过程。

PPO 算法在强化学习领域取得了显著的成功，特别是在连续控制任务上表现出色。许多研究人员将 PPO 应用于语言模型的训练，尤其是在 GPT（Generative Pre-trained Transformer）[GPT] 等大规模语言模型的优化中。在语言模型的训练过程中，PPO 算法可以帮助模型学习到更加准确、连贯、多样化的文本生成策略，提高了模型的性能和泛化能力。

总的来说，PPO 算法通过引入重要性采样和 surrogate objective function 等技术，在强化学习和自然语言处理等领域取得了显著的进展。其稳定性、数据利用率和训练速度的优势，使其成为了当前学术界和工业界广泛采用的优化算法之一。

> 暂时未理解:
>
> 在 LLM 的训练中，使用 PPO 需要同时载入 4 个模型：
>
> - **Actor Model：**用于进化训练的生成模型
> - **Critic Model：**用于进化训练的评判模型
> - **Ref Model：**参照模型，通过 KL 来限制 Actor 模型的训练方向
> - **Reward Model：**奖励模型，用于指导 Actor 进化
>
> 为了节省显存，通常会将 actor / critic 共享一个 backbone，这样需要同时载入 3 个模型。这也是 RL 非常耗卡的一个重要原因。

##### 2.3.2 PPO in LLM 优化目标

1. 在每个token上都计算一个和第一步训练出的生成模型之间的KL-Divergence，其目的是希望不要强化学习的过程中不要太过于偏离最开始的生成模型。
2. PPO-ptx，在训练的同时加入一些通用预训练任务以维持在通用NLP任务上的性能。

$$
\begin{aligned}
\text { objective }(\phi)= & E_{(x, y) \sim D_{\pi_\phi^{\mathrm{RL}}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]+ \\
& \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
\end{aligned}
$$

PPO优化目标:
$$
\begin{aligned}
\text { objective }(\phi)= & E_{(x, y) \sim D_{\pi_\phi^{\mathrm{RL}}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]+ \\
& \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
\end{aligned}
$$
变量解释:
- $\phi$:强化学习策略$\pi_\phi^{\mathrm{RL}}$的参数。
- $x$:输入文本序列。
- $y$:生成的文本序列。
- $D_{\pi_\phi^{\mathrm{RL}}}$:由强化学习策略$\pi_\phi^{\mathrm{RL}}$生成的数据分布。
- $r_\theta(x, y)$:奖励模型,用于评估生成文本$(x, y)$的质量,其中$\theta$为奖励模型的参数。
- $\beta$:平衡因子,控制奖励最大化和KL散度最小化之间的权衡。
- $\pi_\phi^{\mathrm{RL}}(y|x)$:强化学习策略,表示在给定输入$x$的情况下生成输出$y$的概率。
- $\pi^{\mathrm{SFT}}(y|x)$:监督微调策略,表示在给定输入$x$的情况下生成输出$y$的概率。
- $D_{\text {pretrain }}$:预训练数据集,用于确保强化学习策略不会偏离预训练语言模型的分布太远。
- $\gamma$:超参数,控制预训练目标的权重。

公式解析:
PPO优化目标由两部分组成,分别对应着不同的训练目的。

第一部分:
$$
E_{(x, y) \sim D_{\pi_\phi^{\mathrm{RL}}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]
$$
这部分的目的是最大化强化学习策略$\pi_\phi^{\mathrm{RL}}$在由其生成的数据分布$D_{\pi_\phi^{\mathrm{RL}}}$上的期望奖励$r_\theta(x, y)$,同时最小化强化学习策略$\pi_\phi^{\mathrm{RL}}$与监督微调策略$\pi^{\mathrm{SFT}}$之间的KL散度。KL散度用于衡量两个概率分布之间的差异,通过最小化KL散度,可以使强化学习策略不会偏离监督微调策略太远。$\beta$​用于平衡奖励最大化和KL散度最小化之间的权衡。

第二部分:
$$
\gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
$$
这部分的目的是最大化强化学习策略$\pi_\phi^{\mathrm{RL}}$在预训练数据集$D_{\text {pretrain }}$上的对数似然。通过这一项,可以确保强化学习策略在优化过程中不会偏离预训练语言模型的分布太远,从而保留预训练语言模型学到的通用语言知识。$\gamma$用于控制预训练目标的权重。

综上所述,PPO优化目标旨在同时实现以下三个目标:
1. 最大化强化学习策略生成文本的期望奖励。
2. 最小化强化学习策略与监督微调策略之间的差异。
3. 确保强化学习策略不会偏离预训练语言模型的分布太远。

通过平衡这三个目标,PPO算法可以训练出一个能够生成高质量、符合指令要求的文本,同时保留预训练语言模型的通用语言知识的强化学习策略。

> 下面是一个 PPO 训练 LLM 的 pipeline 示例：
>
> 训练数据样例:
> 假设我们有以下训练数据:
>
> 输入(x): "Please translate the following sentence to French: I love natural language processing."
> 期望输出(y): "J'aime le traitement automatique du langage naturel."
> 奖励(r): 0.8 (假设奖励模型给出了0.8的分数,表示生成的译文质量较高)
>
> 模型输入输出细节:
>
> PPO训练阶段:
> - 输入:
>   - 监督微调策略$\pi^{\mathrm{SFT}}$
>   - 奖励模型$r_\theta$
>   - 预训练数据集$D_{\text {pretrain }}$
>   - 强化学习策略$\pi_\phi^{\mathrm{RL}}$生成的数据$(x, y)$,
>     如("Please translate the following sentence to French: I love natural language processing.", "J'adore le traitement du langage naturel.")
> - 输出: 优化后的强化学习策略$\pi_\phi^{\mathrm{RL}}$
>
> 在PPO训练阶段,优化目标函数的作用如下:
> 1. 第一部分: $E_{(x, y) \sim D_{\pi_\phi^{\mathrm{RL}}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]$
>    - 计算强化学习策略$\pi_\phi^{\mathrm{RL}}$生成的数据$(x, y)$的期望奖励$r_\theta(x, y)$,同时最小化强化学习策略与监督微调策略之间的KL散度。
>    - 例如,如果强化学习策略生成的译文为"J'adore le traitement du langage naturel.",奖励模型给出的分数为0.7,则该部分的值为$0.7 - \beta \log(\pi_\phi^{\mathrm{RL}}(y|x) / \pi^{\mathrm{SFT}}(y|x))$。
>
> 2. 第二部分: $\gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]$
>    - 计算强化学习策略在预训练数据集上的对数似然,确保策略不会偏离预训练语言模型的分布太远, 以维持在通用NLP任务上的性能。
>    - 例如,对于预训练数据集中的句子"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.",计算$\log(\pi_\phi^{\mathrm{RL}}(x))$,并将其乘以权重$\gamma$。
>
> 通过优化PPO目标函数,强化学习策略$\pi_\phi^{\mathrm{RL}}$可以学习到如何根据指令生成高质量的输出,同时保留预训练语言模型的通用语言知识。在实际应用中,我们通过多次迭代优化PPO目标函数,不断更新强化学习策略的参数$\phi$,最终得到一个性能优异的语言模型。

##### 2.3.3 PPO 训练稳定性

PPO训练不稳定的原因主要有以下几个方面。

* 首先,在许多任务中,奖励信号非常稀疏,模型难以从环境中获得有意义的反馈,导致学习过程容易陷入局部最优或发散。
* 其次,设计一个合适的奖励函数是非常具有挑战性的,奖励函数需要准确反映任务目标,同时避免引入偏差或误导模型。\
* 再者,PPO通过限制策略更新幅度来提高训练稳定性,但如果限制过于严格或过于宽松,都可能导致训练速度减慢或策略更新不稳定。
* 此外,强化学习通常需要大量的环境交互样本,样本效率低会导致训练时间长,计算成本高。
* 最后,PPO算法包含多个超参数,这些超参数的选择对训练稳定性和性能有显著影响,调节难度大。

为了应对这些挑战,研究者们提出了多种方法来提高PPO训练的稳定性。

* 其中,奖励函数优化是一个重要的方向,通过仔细设计奖励函数,引入辅助奖励等方式,可以为模型提供更加明确和有效的学习信号。
* 策略正则化是另一个常用的技巧,在目标函数中引入KL散度惩罚或熵奖励等正则化项,可以鼓励策略的探索性和多样性,提高训练稳定性。
* 同时,PPO算法本身通过限制策略更新幅度来提高稳定性,进一步优化信任区域,如使用自适应的KL惩罚系数或采用近端信任区域优化(TRPO)等方法,也能够增强训练的稳定性。

在实际应用中,并行化训练是提高PPO训练效率的重要手段。通过并行化环境交互和策略更新,可以显著提高样本效率和训练速度。常见的并行化方法包括异步优势演员评论家(A3C)、近端策略优化(PPO)的并行版本等。此外,使用自动超参数优化方法,如贝叶斯优化、随机搜索等,来寻找最优的超参数组合,也能够减轻手动调参的负担,提高训练效率。

除了优化训练算法,改进模型架构也是提高PPO训练稳定性和性能的重要途径。设计更加有效的策略网络架构,如使用注意力机制、记忆增强等技术,可以提高模型的表达能力和泛化性能。同时,将特定领域的先验知识融入到模型设计和训练过程中,如引入专家经验、规则约束等,能够加速训练并提高稳定性。

> #### 稳定性调参经验
>
> 首先,适当**调大KL散度惩罚系数可以有效稳定训练过程**。KL散度惩罚是近端策略优化(PPO)算法中的一个关键组件,它通过限制策略更新幅度来防止策略偏离先前的分布太远。通过增大KL散度惩罚系数,可以更好地约束策略更新,减少剧烈波动。此外,采用动态调整KL系数的策略,根据训练过程中的表现自适应地调整惩罚强度,也是一种优化训练稳定性的有效方法。
>
> 其次,**使用一个稳定、可靠的奖励模型(RM)对于确保训练稳定至关重要**。奖励模型负责评估生成文本的质量,并为强化学习算法提供反馈信号。一个设计良好的奖励模型应当准确反映任务目标,提供有意义的学习信号,同时尽量避免引入偏差或噪声。选择或构建一个稳定的奖励模型,可以显著提高训练的稳定性和效率。
>
> 再者,**奖励归一化**是另一个影响训练稳定性的关键因素。将奖励值归一化到一个合适的范围内,可以帮助模型更好地理解和利用反馈信号,加速收敛过程。常见的奖励归一化方法包括线性缩放、标准化等。通过恰当的奖励归一化,可以有效提高训练的稳定性和效率。
>
> 此外,适当**增大批量大小(batch size)**也有助于训练稳定。较大的批量大小可以减少梯度估计的方差,提供更加稳定和可靠的梯度信息。但是,批量大小的选择需要权衡计算效率和内存消耗,过大的批量大小可能导致内存不足或训练时间过长。因此,需要根据具体的任务和硬件条件,选择一个合适的批量大小。
>
> 在训练过程中,有时会出现"奖励骇客"(reward hacking)的情况,即模型通过发现一些捷径或漏洞来获得高奖励,但实际上并没有真正学习到有意义的策略。为了防止这种情况,一个有效的方法是**结合多个奖励模型进行打分**。通过综合考虑不同奖励模型的评分,可以减少单一奖励模型的局限性,提高奖励信号的可靠性。例如,Llama 2模型同时使用了安全性(Safety)和有用性(Helpful)两个奖励模型,以平衡不同的任务目标,防止模型过度关注某一方面而忽略其他重要因素。
>
> 总的来说,调参是训练LLM的一个关键环节,需要研究者和实践者根据具体任务和数据特点,灵活选择和组合不同的优化策略。通过适当调整KL散度惩罚系数、选择稳定的奖励模型、进行奖励归一化、增大批量大小以及结合多个奖励模型等方法,可以有效提高训练的稳定性和效率。同时,持续探索和创新,开发更加先进、高效的调参技术,对于推动LLM的进一步发展具有重要意义。只有不断优化训练策略,突破算法瓶颈,才能充分发挥LLM在自然语言处理领域的巨大潜力,为人工智能技术的进步做出贡献。